"Tweet id","Tweet permalink","Tweet text","time","impressions","engagements","engagement rate","retweets","replies","likes","user profile clicks","url clicks","hashtag clicks","detail expands","permalink clicks","app opens","app installs","follows","email tweet","dial phone","media views","media engagements","promoted impressions","promoted engagements","promoted engagement rate","promoted retweets","promoted replies","promoted likes","promoted user profile clicks","promoted url clicks","promoted hashtag clicks","promoted detail expands","promoted permalink clicks","promoted app opens","promoted app installs","promoted follows","promoted email tweet","promoted dial phone","promoted media views","promoted media engagements"
"1432793754507923461","https://twitter.com/LisaDeBruine/status/1432793754507923461","@tmalsburg I agree your solution is more efficient. But brute force takes less than a millisecond for the common use cases. Your solution is more complex to code and I think would run into the same problem with low items high repeats sets.","2021-08-31 19:53 +0000","89.0","5.0","0.056179775280898875","0.0","1.0","0.0","2.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432747650101747715","https://twitter.com/LisaDeBruine/status/1432747650101747715","@tjmahr @emma_cogdev Thanks! I've implemented a pretty fast brute-force version in JS, but their code will help with other pseudo-random constraints.

(This is just to speed test, the real implementation will have arbitrary items and repeats can vary between items.)

https://t.co/khtKsumLxX","2021-08-31 16:50 +0000","87.0","5.0","0.05747126436781609","0.0","1.0","1.0","0.0","2.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432745435295371269","https://twitter.com/LisaDeBruine/status/1432745435295371269","@tmalsburg This is my solution so far (brute force but really fast in JS), but that might solve the problem with 2 items and many repeats, which is trivial (there are only 2 permissible orders) but my generic solution doesn't work well.

https://t.co/khtKsumLxX","2021-08-31 16:41 +0000","110.0","10.0","0.09090909090909091","0.0","1.0","1.0","0.0","7.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432744853415337986","https://twitter.com/LisaDeBruine/status/1432744853415337986","@Hao_and_Y It's not elegant (yet), but the JS version is FAST

https://t.co/khtKsumLxX","2021-08-31 16:39 +0000","44.0","13.0","0.29545454545454547","0.0","1.0","1.0","0.0","3.0","0.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432731306572582917","https://twitter.com/LisaDeBruine/status/1432731306572582917","@Hao_and_Y Good catch! I'll have to detect the 2 items case and handle that separately. The simulation was maxing out at 100K iterations about 30% of the time for 2 items repeated 10 times each. 3 items 10 times each maxed out 20% of the time. 4 items 10 times each only did 4%.","2021-08-31 15:45 +0000","31.0","3.0","0.0967741935483871","0.0","1.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432729621129269254","https://twitter.com/LisaDeBruine/status/1432729621129269254","@Hao_and_Y I agree, but it's a reasonable strategy for pseudo-random ordering of stimuli. I might just implement it when the brute force version can't be found quickly.","2021-08-31 15:38 +0000","14.0","1.0","0.07142857142857142","0.0","0.0","0.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432728989550972931","https://twitter.com/LisaDeBruine/status/1432728989550972931","@Lincoln81 Yup (check another branch of this thread for the code). I think I've decided to use a hybrid approach and detect if it's a regular sequence (X repeats of Y items) and either concatenate X random orders of the unique items (checking end repeats) or use the brute force approach.","2021-08-31 15:36 +0000","60.0","2.0","0.03333333333333333","0.0","1.0","0.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432728119279071239","https://twitter.com/LisaDeBruine/status/1432728119279071239","@Hao_and_Y Maybe I can detect the regular cases where each item is repeated M times (probably most common but not constrained) and make M random orders with the constraint that the last item of one isn't the same as the first item of the next. And brute force otherwise.","2021-08-31 15:32 +0000","25.0","5.0","0.2","0.0","1.0","0.0","1.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432727549298290692","https://twitter.com/LisaDeBruine/status/1432727549298290692","@Hao_and_Y I thin there should be a *possible* sequence for any list where no item is repeated more than N/2 times (it can always go ABACADAE...) but it might take a LOT of random iterations if there are a larger number of items repeated a large number of times.","2021-08-31 15:30 +0000","38.0","4.0","0.10526315789473684","0.0","2.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432726872446717958","https://twitter.com/LisaDeBruine/status/1432726872446717958","@Hao_and_Y When I tested this with 100 items 10 times each, the number of iterations needed was up to 30K, so I'll have to see how slow the computation is in JS and put a max number of iterations on it.","2021-08-31 15:27 +0000","29.0","2.0","0.06896551724137931","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432726290357002243","https://twitter.com/LisaDeBruine/status/1432726290357002243","@Lincoln81 I have to implement this in JavaScript for arbitrary lists of stimuli. From a few simulations in R, it seldom takes more than 200 tries with 10 items repeated 5 times each. But with 100 items 10 times each, it can go up to 30K.","2021-08-31 15:25 +0000","71.0","4.0","0.056338028169014086","0.0","1.0","0.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432723418814816260","https://twitter.com/LisaDeBruine/status/1432723418814816260","@Hao_and_Y This seems to work in R (now to translate to JS)

x &lt;- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4)
n &lt;- length(x)

while (any(x[1:(n-1)] == x[2:n])) {
  x &lt;- sample(x)
}","2021-08-31 15:14 +0000","48.0","5.0","0.10416666666666667","0.0","1.0","1.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432722051622653953","https://twitter.com/LisaDeBruine/status/1432722051622653953","@speaktomo Like if you want to show 10 stimuli 5 times each, but have no repeated trials.","2021-08-31 15:08 +0000","42.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432720420181053455","https://twitter.com/LisaDeBruine/status/1432720420181053455","@Lincoln81 You can back yourself into a corner with this method. For example, from the sequence AAABBBCCC, if you allocate BCABCBC as the first 7, then there is no way to finish without repeats.","2021-08-31 15:02 +0000","142.0","3.0","0.02112676056338028","0.0","1.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432719609099128834","https://twitter.com/LisaDeBruine/status/1432719609099128834","@tegladwintmp2 Sorry, my python isn't great, but I don't think that solves my problem (which I didn't explain well).

Imagine a sequence like AAABBBCCC and I want an order with no repeats, so ABCBABCAC is fine, but ABCABCCBA is not. But the sequence can be anything.","2021-08-31 14:59 +0000","32.0","2.0","0.0625","0.0","1.0","0.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432718891680157712","https://twitter.com/LisaDeBruine/status/1432718891680157712","@Hao_and_Y I think I've not specified the problem clearly. 

For example, if I have 20 trials of 5 stimuli 4 times each and want to show them in a ""random"" order with the constraint that the same stimulus never be shown twice. But users can specify any number of stimuli with any repeats.","2021-08-31 14:56 +0000","50.0","4.0","0.08","0.0","1.0","0.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432718223145852928","https://twitter.com/LisaDeBruine/status/1432718223145852928","@emma_cogdev Ooh, that's a great reference! Mix does a lot more than I need to, but all of the constraints they cover might be worth building into experimentum someday. Sadly, the code itself is no longer at the link in the abstract.

https://t.co/h5x5r2X9kw","2021-08-31 14:53 +0000","180.0","13.0","0.07222222222222222","0.0","2.0","1.0","0.0","3.0","0.0","7.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432716941362356230","https://twitter.com/LisaDeBruine/status/1432716941362356230","@tmorris_mrc Users will be able to specify a sequence of any number (practically less than 1000) with as many duplicates as they want (as long as none are &gt; N/2). So the stimuli could be 10 As, 5Bs, 5Cs, and 1D. Although they are more likely to be the same number of repeats of each stimulus.","2021-08-31 14:48 +0000","141.0","5.0","0.03546099290780142","0.0","1.0","1.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432710555056414734","https://twitter.com/LisaDeBruine/status/1432710555056414734","P.S., I'll probably be implementing this in PHP or JavaScript, so I don't need R code, but could figure out how to convert.","2021-08-31 14:23 +0000","1015.0","34.0","0.033497536945812804","0.0","2.0","0.0","3.0","0.0","0.0","29.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432710011021578246","https://twitter.com/LisaDeBruine/status/1432710011021578246","Does anyone have tips for an algorithm that will generate a ""random"" order with no repeats from a list of N items where any item can be present up to N/2 times?

Should I just generate random orders until one fills this criterion, or is there a more principled method?","2021-08-31 14:20 +0000","3257.0","156.0","0.04789683758059564","2.0","10.0","2.0","9.0","1.0","0.0","131.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432681505285971969","https://twitter.com/LisaDeBruine/status/1432681505285971969","@ShuhBillSkee Is the “demise of guys” meant to be a negative outcome?","2021-08-31 12:27 +0000","471.0","17.0","0.036093418259023353","0.0","1.0","5.0","2.0","0.0","0.0","9.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432357337621319682","https://twitter.com/LisaDeBruine/status/1432357337621319682","@echodroff @duane_g_watson @davidrfeinberg any ideas?","2021-08-30 14:59 +0000","416.0","6.0","0.014423076923076924","0.0","0.0","0.0","4.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432357035824267266","https://twitter.com/LisaDeBruine/status/1432357035824267266","@dh4gan @forganross Is Jagger pronounced like Jaegermeister or Mick Jagger?","2021-08-30 14:58 +0000","34.0","3.0","0.08823529411764706","0.0","1.0","0.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432331824131448839","https://twitter.com/LisaDeBruine/status/1432331824131448839","The contrast functions are meant to be an easy way to recode factors, although working on that sent me down the rabbit hole of jingle-jangle contrast terminology. I need a second opinion on both accuracy and clarity here.

https://t.co/zmqRzLjv2q https://t.co/2uJ2XuVqSS","2021-08-30 13:18 +0000","601.0","24.0","0.03993344425956739","0.0","0.0","1.0","1.0","2.0","0.0","3.0","0.0","0","0","0","0","0","17","17","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432331814618734592","https://twitter.com/LisaDeBruine/status/1432331814618734592","I'm trying a new way to build up a mixed design, sort of like how a ggplot is built up by layers, where you can specify the random effects structure, add fixed factors, recode them, and add random effects by parameter.

https://t.co/VXApEKvAdg https://t.co/fzWDBoIpba","2021-08-30 13:18 +0000","705.0","59.0","0.08368794326241134","0.0","2.0","2.0","0.0","4.0","0.0","10.0","0.0","0","0","0","0","0","41","41","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432331802505515013","https://twitter.com/LisaDeBruine/status/1432331802505515013","Does anyone want to download the development version of #rstats faux, go through one of the vignettes in the thread, and let me know what they think about the new functions? (Or check it doesn't break your old scripts?)

devtools::install_github(""debruine/faux"")","2021-08-30 13:18 +0000","1383.0","33.0","0.02386117136659436","2.0","1.0","0.0","0.0","0.0","0.0","30.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432325774988677124","https://twitter.com/LisaDeBruine/status/1432325774988677124","@economeager Definitely a win. I am constantly looking up how to do stuff in books or tutorials that I wrote myself. Once I tried to upvote a stackoverflow response that exactly answered my question but was told I couldn’t, because I wrote it 🤦","2021-08-30 12:54 +0000","987.0","23.0","0.02330293819655522","1.0","0.0","10.0","7.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432314202048417796","https://twitter.com/LisaDeBruine/status/1432314202048417796","@bvancil @GootjesFrank Ah, I really need to get more familiar with the debugging tools. I've only just gotten to grips with browser() in shiny. Thanks!","2021-08-30 12:08 +0000","29.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432302255135469570","https://twitter.com/LisaDeBruine/status/1432302255135469570","@bvancil I'm not quite sure how to use that to solve my problem. I often have functions that I need to step through line by line to debug, but the lines with code like list(...) won't run unless I re-write them. I think I just want to be able to set ... in the global environment.","2021-08-30 11:20 +0000","58.0","4.0","0.06896551724137931","0.0","1.0","0.0","2.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432283612574097410","https://twitter.com/LisaDeBruine/status/1432283612574097410","Does anyone have a good solution for debugging #rstats functions that use ... ? I keep replacing it with test code in the function and then forgetting to change it back (so all my tests break).","2021-08-30 10:06 +0000","2412.0","73.0","0.030265339966832505","2.0","3.0","1.0","5.0","0.0","2.0","60.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432082257846624260","https://twitter.com/LisaDeBruine/status/1432082257846624260","@DrGBuckingham That’s not a critique of the method, which looks really good (and kudos to the authors for being aware of the potential for bias). It’s just a caution that this isn’t going to let you conclusively ID people on blurry CCTV footage. Squinting at a blurry image is just as accurate.","2021-08-29 20:46 +0000","169.0","9.0","0.05325443786982249","0.0","0.0","6.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432081681092067338","https://twitter.com/LisaDeBruine/status/1432081681092067338","@DrGBuckingham The real test is with familiar faces. These sorts of reconstructions tend to look great for unfamiliar faces and totally fail to reconstruct known people (as you’d expect; you can’t magic specific info out of noise, only representative info).","2021-08-29 20:44 +0000","790.0","58.0","0.07341772151898734","0.0","1.0","19.0","3.0","0.0","0.0","35.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1432054786988380170","https://twitter.com/LisaDeBruine/status/1432054786988380170","@kierisi I always read this as neuro-linguistic programming first and have to really think about the non-sus meaning.","2021-08-29 18:57 +0000","337.0","6.0","0.017804154302670624","0.0","1.0","2.0","2.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431963917320916995","https://twitter.com/LisaDeBruine/status/1431963917320916995","@jephjacques why must you make me choose between rainbow and numerical order?
❤️🧡💛💚💙💜 https://t.co/epaCwT6xlD","2021-08-29 12:56 +0000","40.0","1.0","0.025","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431702886820597762","https://twitter.com/LisaDeBruine/status/1431702886820597762","@EdinburghKnee I’ve been here 18 years and no idea either.","2021-08-28 19:39 +0000","558.0","6.0","0.010752688172043012","0.0","1.0","2.0","1.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431655026817179650","https://twitter.com/LisaDeBruine/status/1431655026817179650","@DrGBuckingham @jamesheathers @hertzpodcast https://t.co/5PK81Ti9ZQ","2021-08-28 16:28 +0000","140.0","8.0","0.05714285714285714","0.0","0.0","2.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","50","4","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431649631486857216","https://twitter.com/LisaDeBruine/status/1431649631486857216","@jamesheathers @hertzpodcast To be fair, impact factors *are* maddening. Has anyone tried to put them out of business by making an open source, replicable, highly correlated metric? Better yet, a suite of metrics that are too hard to game more than a few.","2021-08-28 16:07 +0000","214.0","7.0","0.03271028037383177","0.0","1.0","4.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431649199175708677","https://twitter.com/LisaDeBruine/status/1431649199175708677","@jamesheathers @hertzpodcast You’re even rantier in #136 :) 

(I listened while cycling to Loch Lomond) https://t.co/tNPEauMEUx","2021-08-28 16:05 +0000","300.0","11.0","0.03666666666666667","0.0","1.0","2.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","8","8","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431624526962511872","https://twitter.com/LisaDeBruine/status/1431624526962511872","@d_olivaw I just got to that part 😂 (it does make me feel a little queasy) https://t.co/H07B5m22X1","2021-08-28 14:27 +0000","39.0","2.0","0.05128205128205128","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431622419941298188","https://twitter.com/LisaDeBruine/status/1431622419941298188","I think I’m going to get a lot of use out of the sloop package. I often find myself poking about the internals of S3 functions trying to figure out how they work.

https://t.co/vlDaqOLffc","2021-08-28 14:19 +0000","509.0","4.0","0.007858546168958742","0.0","0.0","0.0","0.0","2.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431618790803099652","https://twitter.com/LisaDeBruine/status/1431618790803099652","I love when I realise I do actually understand a concept when I wonder “why not do it this way?” and the next paragraph is why to do it that way :) @hadleywickham writes in a way that causes this a lot (and I hope I can learn to emulate).","2021-08-28 14:04 +0000","705.0","5.0","0.0070921985815602835","0.0","1.0","2.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431618789301436418","https://twitter.com/LisaDeBruine/status/1431618789301436418","Reading coding books on cycling breaks is becoming a bit of a tradition. I’m back at Bowling Harbour reading Advanced R.","2021-08-28 14:04 +0000","433.0","3.0","0.006928406466512702","0.0","1.0","2.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431617123835580417","https://twitter.com/LisaDeBruine/status/1431617123835580417","@PF_Hitchcock @EikoFried I use the keyboard shortcut for inserting roxygen documentation (bunch-of-keys-R). It’s another one I can do with my hands but not my brain (and I’m on the phone right now).","2021-08-28 13:58 +0000","190.0","7.0","0.03684210526315789","0.0","1.0","2.0","1.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431605945138917380","https://twitter.com/LisaDeBruine/status/1431605945138917380","@gkampolis @bmwiernik Now I will always be able to remember it :) And I now have a use for all the pictures I took at the Magritte museum in Brussels! I hope the students will appreciate an art history lesson in their data skills class.","2021-08-28 13:13 +0000","54.0","3.0","0.05555555555555555","0.0","0.0","2.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431604760231165964","https://twitter.com/LisaDeBruine/status/1431604760231165964","One episode of @hertzpodcast is pretty much the perfect length for a cycle out to Bowling Marina. James was very ranty about auxiliary hypotheses in #135 Also I learned his brain is full of lizard eggs 🦎 https://t.co/A9YKL0bfB2","2021-08-28 13:09 +0000","4213.0","170.0","0.040351293615001185","1.0","1.0","23.0","33.0","0.0","0.0","50.0","0.0","0","0","0","0","0","62","62","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431555459677315077","https://twitter.com/LisaDeBruine/status/1431555459677315077","@livin_emma I love this 😍 I, too, need to make a keyboard shortcut for opening a new Rmd (with a custom template). https://t.co/DnNTX7pQeB","2021-08-28 09:53 +0000","314.0","13.0","0.041401273885350316","0.0","0.0","0.0","0.0","1.0","0.0","0.0","0.0","0","0","0","0","0","12","12","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431553799899033606","https://twitter.com/LisaDeBruine/status/1431553799899033606","@jottinog That’s one that took me a weirdly long time to be able to use without looking up, given how useful it is for debugging and development.","2021-08-28 09:46 +0000","151.0","2.0","0.013245033112582781","0.0","0.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431553484688605186","https://twitter.com/LisaDeBruine/status/1431553484688605186","@samuel_marsh My R restart is Cmd-R (I use it constantly). Datapasta sounds really cool, but I’ve only had a very few times I’ve needed it. In what context do you copy/paste tables and vectors a lot?","2021-08-28 09:45 +0000","296.0","3.0","0.010135135135135136","0.0","1.0","0.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431552940188344321","https://twitter.com/LisaDeBruine/status/1431552940188344321","@seizethedatum You might be able to generate a PDF with these styles using pagedjs-cli (see https://t.co/TSwF18OmCT). I’m going to experiment with this a bit because I hate having to knit my books twice for html and pdf versions.","2021-08-28 09:43 +0000","53.0","5.0","0.09433962264150944","0.0","1.0","1.0","0.0","3.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431550074279038982","https://twitter.com/LisaDeBruine/status/1431550074279038982","@burrbenjamin1 @thomas_mock Ctrl-shift-R to insert a code section in RMarkdown is my most frequently used shortcut, but the second I think about it, like to explain to students how to do it, I have no idea. My fingers only know when I’m not thinking about it.","2021-08-28 09:31 +0000","229.0","4.0","0.017467248908296942","0.0","0.0","2.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431549550863454213","https://twitter.com/LisaDeBruine/status/1431549550863454213","@deevybee @vascobrazao Yeah, I can never get it to work, but it would be really useful on occasion. I’m going to have to look at my key bindings. I might write myself a blog post collating the best of these tips so I don’t forget them!","2021-08-28 09:29 +0000","265.0","4.0","0.01509433962264151","0.0","0.0","3.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431346592796266500","https://twitter.com/LisaDeBruine/status/1431346592796266500","@StijnDvos I mapped that onto cmd-R, I use it so often!","2021-08-27 20:03 +0000","87.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431260427669889028","https://twitter.com/LisaDeBruine/status/1431260427669889028","Oops! There's an error in that YAML header, it should be:

---
output:
  html_document:
    theme: 
      version: 4
      bootswatch: yeti
--- 

Thanks for catching it, @LeafyEricScott!","2021-08-27 14:20 +0000","1153.0","13.0","0.011274934952298352","1.0","0.0","6.0","3.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431259891470057474","https://twitter.com/LisaDeBruine/status/1431259891470057474","@LeafyEricScott Sorry! I have no idea why I wrote theme instead of bootswatch in my original tweet.","2021-08-27 14:18 +0000","30.0","2.0","0.06666666666666667","0.0","0.0","1.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431256059549716488","https://twitter.com/LisaDeBruine/status/1431256059549716488","@LeafyEricScott I’m not sure if you need {bslib}. I thought this just worked with the newest rmarkdown, but I didn’t try in just RMarkdown until after I’d updated bookdown, which I think requires bslib.","2021-08-27 14:03 +0000","143.0","4.0","0.027972027972027972","0.0","1.0","0.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431242314630971392","https://twitter.com/LisaDeBruine/status/1431242314630971392","@Nate__Haines Thanks! For me, crtl-shift-alt-left and -right move between tabs in the source pane.

And ctrl-shift-alt-up and -down increase and decrease selected text to the next boundaries. That's really useful!","2021-08-27 13:08 +0000","629.0","2.0","0.003179650238473768","0.0","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431238007420559362","https://twitter.com/LisaDeBruine/status/1431238007420559362","@_chenson__ It's very easy to use vim effectively

(esc) :q","2021-08-27 12:51 +0000","55.0","2.0","0.03636363636363636","0.0","0.0","0.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431237685532893191","https://twitter.com/LisaDeBruine/status/1431237685532893191","@bmwiernik +1 for ctrl-shift-A. I don't use it enough yet to always remember it, and sometimes I disagree with the formatting choices, but I'm coming around to just acquiescing to the auto-format for the sake of consistency.","2021-08-27 12:50 +0000","292.0","2.0","0.00684931506849315","0.0","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431237223807131651","https://twitter.com/LisaDeBruine/status/1431237223807131651","@bmwiernik I have to say I'm mystified how so many people have trouble typing %&gt;% manually (it's just muscle memory for me at this point), but this feels like one that students will appreciate. What's a good mnemonic for remembering why M and not P? I read the pipe as ""and then"".","2021-08-27 12:48 +0000","1077.0","23.0","0.02135561745589601","0.0","1.0","1.0","7.0","0.0","0.0","14.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431236539539931147","https://twitter.com/LisaDeBruine/status/1431236539539931147","@_chenson__ Thanks :) but I'm hoping to make a short list of the most useful commands to add to my repertoire. I know there are hundreds, and if I try to add them all at once, nothing will stick. I'm also trying to decide which I should teach to students without overwhelming them.","2021-08-27 12:45 +0000","65.0","2.0","0.03076923076923077","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431235240412995590","https://twitter.com/LisaDeBruine/status/1431235240412995590","@ecsalomon @Alex_Danvers OMG this one is going to change my life! https://t.co/q9pjbQIKlw","2021-08-27 12:40 +0000","366.0","16.0","0.04371584699453552","0.0","1.0","3.0","0.0","0.0","0.0","8.0","0.0","0","0","0","0","0","184","4","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431234901014106115","https://twitter.com/LisaDeBruine/status/1431234901014106115","@camjpatrick I'm on a mac and I think these were default for me. I don't remember ever having to change it. It's a command that works almost everywhere on my mac.

These differences make it really hard to teach R to students with different operating systems.","2021-08-27 12:39 +0000","655.0","7.0","0.010687022900763359","0.0","1.0","1.0","4.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431234132508545025","https://twitter.com/LisaDeBruine/status/1431234132508545025","@vascobrazao It's cmd-alt-arrow to duplicate lines for me (mac)

I've been meaning to learn how to do that editing several lines thing, but I use Magnet to organise windows and it's overridden ctrl-alt-arrow, so I might have to remap that.","2021-08-27 12:36 +0000","335.0","3.0","0.008955223880597015","0.0","0.0","1.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431232609540087809","https://twitter.com/LisaDeBruine/status/1431232609540087809","@jvcasill Cmd-d deletes a whole line for me!
Ctrl-d delete the selected text.
Cmd-shift-D duplicates the selected text.
Opt/alt-d inserts a lowercase delta.

(on a mac)","2021-08-27 12:30 +0000","706.0","11.0","0.015580736543909348","0.0","1.0","2.0","3.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431231435340398592","https://twitter.com/LisaDeBruine/status/1431231435340398592","@marzsv There is only one vim command I need to know, and it's on a sticky note on the wall next to my computer because I can never remember it when I need it (which is anytime I accidentally end up in vim). https://t.co/yohdMtZwgr","2021-08-27 12:25 +0000","1566.0","218.0","0.1392081736909323","1.0","1.0","8.0","3.0","2.0","0.0","17.0","0.0","0","0","0","0","0","186","186","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431230368473100297","https://twitter.com/LisaDeBruine/status/1431230368473100297","@mickcraig101 I love tab-complete for function names and argument. Also the list of arguments and definition that you can see if you tab inside a function's parentheses.","2021-08-27 12:21 +0000","409.0","5.0","0.012224938875305624","0.0","1.0","1.0","2.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431229207015796739","https://twitter.com/LisaDeBruine/status/1431229207015796739","@Don_lyall ctrl-b just moves my cursor back one space!","2021-08-27 12:16 +0000","281.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431228808003260418","https://twitter.com/LisaDeBruine/status/1431228808003260418","I want to learn more keyboard commands to reduce my mouse use, especially when coding in RStudio. I already love ctrl-A and ctrl-E for jumping to the start and end of a line. What are others can't you live without? (both generic terminal and RStudio-specific welcome)","2021-08-27 12:15 +0000","45867.0","2558.0","0.055769943532387116","35.0","43.0","261.0","111.0","6.0","0.0","2034.0","0.0","0","0","3","0","0","65","65","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431222063423971331","https://twitter.com/LisaDeBruine/status/1431222063423971331","The thumbnail theme examples at https://t.co/yQu3N6EhsW are beautiful, but I hardly ever use buttons in an Rmd and just wanted to know what headers, links, lists, inline code, and code blocks look like.","2021-08-27 11:48 +0000","1266.0","37.0","0.029225908372827805","1.0","0.0","2.0","0.0","26.0","0.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431222058973814785","https://twitter.com/LisaDeBruine/status/1431222058973814785","I've only just (probably late) learned how to change the theme of an R Markdown document to use bootstrap4. I made myself this comparison guide and thought it might be helpful for others, too.
---
output:
  html_document:
    theme: 
      version: 4
      theme: yeti
--- https://t.co/w2szxXkPqt","2021-08-27 11:48 +0000","14451.0","1643.0","0.1136945540101031","14.0","5.0","137.0","32.0","9.0","0.0","312.0","0.0","0","0","0","0","0","1134","1134","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431149643509927936","https://twitter.com/LisaDeBruine/status/1431149643509927936","@JamesEBartlett @IrisHolzleitner has experience with this in Glasgow and might have some advice.","2021-08-27 07:00 +0000","508.0","11.0","0.021653543307086614","0.0","1.0","0.0","7.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431149209844060163","https://twitter.com/LisaDeBruine/status/1431149209844060163","@psforscher @chbergma @lakens @dstephenlindsay @cconrymurray I keep meaning to do a blog post on simr. I like to manually sim because it helps with understanding and other aspects of analysis planning. But simr is more straightforward for power if you have parameters from pilot data, although simulating a new data structure can be tricky.","2021-08-27 06:58 +0000","234.0","8.0","0.03418803418803419","0.0","0.0","6.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431146505797316610","https://twitter.com/LisaDeBruine/status/1431146505797316610","@fusaroli @chbergma @cconrymurray @chaddlewick @shirilevari @salonikrishnan Yeah, you can use the pilot study to at least get your random effects parameters in the right order of magnitude (I’m not sure it’s possible to develop an intuition for guessing them without data) and then check power with more or less conservative values.","2021-08-27 06:48 +0000","181.0","12.0","0.06629834254143646","0.0","1.0","5.0","0.0","1.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431145089452150787","https://twitter.com/LisaDeBruine/status/1431145089452150787","@westwoodsam1 @UoWPsych @UniWestminster @lingx2lam Congratulations!","2021-08-27 06:42 +0000","376.0","8.0","0.02127659574468085","0.0","0.0","1.0","2.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431004089140355081","https://twitter.com/LisaDeBruine/status/1431004089140355081","@shirilevari Suggest that category to Chloe! There are definitely days when I cannot handle another coming out focussed plot.

The best queer films I’ve seen lately are Suicide Kale and Booksmart.","2021-08-26 21:22 +0000","248.0","3.0","0.012096774193548387","0.0","1.0","0.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1431002949057236992","https://twitter.com/LisaDeBruine/status/1431002949057236992","@fusaroli @chbergma @cconrymurray @chaddlewick @shirilevari @salonikrishnan That’s a very good question! I guess you could investigate it with dat from some high-n studies by pulling a sub sample as a “pilot” and seeing how big that needs to be. I’m sure there’s a relationship to the design.","2021-08-26 21:17 +0000","268.0","15.0","0.055970149253731345","0.0","2.0","6.0","2.0","1.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430996765092745221","https://twitter.com/LisaDeBruine/status/1430996765092745221","@chbergma @cconrymurray @chaddlewick @shirilevari @salonikrishnan But the thing I want to caution you about is that you need to have realistic values for the random effects standard deviations to set up a useful power analysis. And the only way you can get that is with pilot data using the same or nearly identical design and DV.","2021-08-26 20:53 +0000","620.0","21.0","0.03387096774193549","1.0","1.0","6.0","1.0","0.0","0.0","12.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430995296624992257","https://twitter.com/LisaDeBruine/status/1430995296624992257","@hardsci @JoeHilgard I do think the benefits of having a better understanding of how to fake (simulate) data outweigh the risk of more competent frauds. It really is a necessary step in any useful registered report analysis. We need to remove the systemic reasons to cheat (which RRs also help).","2021-08-26 20:47 +0000","524.0","8.0","0.015267175572519083","0.0","1.0","2.0","1.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430993528709079047","https://twitter.com/LisaDeBruine/status/1430993528709079047","@RichardLehman1 What symptoms look like on people with darker skin. @malone_mk does great work on this.","2021-08-26 20:40 +0000","747.0","8.0","0.0107095046854083","1.0","0.0","4.0","3.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430992276516388868","https://twitter.com/LisaDeBruine/status/1430992276516388868","@chbergma @cconrymurray @chaddlewick @shirilevari @salonikrishnan And here are all the materials for that workshop

https://t.co/NaFI7kmyND","2021-08-26 20:35 +0000","294.0","14.0","0.047619047619047616","0.0","1.0","2.0","0.0","11.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430991128522862594","https://twitter.com/LisaDeBruine/status/1430991128522862594","@chbergma @cconrymurray @chaddlewick I’m not sure if my workshop on simulation for mixed models at the #RHOpenScience2021 series got recorded? @shirilevari or @salonikrishnan might know.","2021-08-26 20:30 +0000","279.0","11.0","0.03942652329749104","0.0","2.0","1.0","2.0","2.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430989475551793153","https://twitter.com/LisaDeBruine/status/1430989475551793153","@chbergma @cconrymurray And @chaddlewick’s talk recorded here might be of use: https://t.co/adStpUGAEU","2021-08-26 20:24 +0000","264.0","20.0","0.07575757575757576","0.0","1.0","4.0","2.0","10.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430989055081254915","https://twitter.com/LisaDeBruine/status/1430989055081254915","@chbergma @cconrymurray There are resources here aimed at understanding how to interpret mixed effects models through simulating them, which then leads to power analyses. 

https://t.co/MtF513dmt4","2021-08-26 20:22 +0000","581.0","60.0","0.10327022375215146","1.0","1.0","15.0","5.0","23.0","0.0","15.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430988148033654787","https://twitter.com/LisaDeBruine/status/1430988148033654787","@hardsci @sharoz Your intent was very clear from an in-group perspective, but I see what you mean.","2021-08-26 20:18 +0000","172.0","14.0","0.08139534883720931","0.0","0.0","2.0","1.0","0.0","0.0","11.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430984369657765892","https://twitter.com/LisaDeBruine/status/1430984369657765892","@sharoz @hardsci Ooh, what happened with the original post?","2021-08-26 20:03 +0000","147.0","15.0","0.10204081632653061","0.0","1.0","0.0","0.0","1.0","0.0","13.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430983916341678091","https://twitter.com/LisaDeBruine/status/1430983916341678091","@_nazeefatima It can be ethical to refuse a reference if you know the reference will harm the candidate. But you should tell them the truth — that you can’t write an honest review that would help their chances — and this is really hard to say.

But that definitely doesn’t seem the case for OP.","2021-08-26 20:02 +0000","20.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430977168117542918","https://twitter.com/LisaDeBruine/status/1430977168117542918","@westwoodsam1 I like to review an outline of each section before it’s written, then a single round of detailed comments one they’re written. But I check methods and results again after that just for errors until there are none.","2021-08-26 19:35 +0000","705.0","4.0","0.005673758865248227","0.0","0.0","2.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430915038269804546","https://twitter.com/LisaDeBruine/status/1430915038269804546","Starting in 2 minutes 
❤️🧡💛💚💙💜","2021-08-26 15:28 +0000","872.0","8.0","0.009174311926605505","0.0","0.0","2.0","0.0","0.0","0.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430905694480322564","https://twitter.com/LisaDeBruine/status/1430905694480322564","@tjmahr Oh, you’ve reminded me to add across() to my updated MSc class. That’s a really nice example, too!","2021-08-26 14:51 +0000","378.0","9.0","0.023809523809523808","0.0","0.0","2.0","3.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430883317830897666","https://twitter.com/LisaDeBruine/status/1430883317830897666","I'll be working on the faux simulation #shiny app today from 16:30 to 18:30 (GMT+1) on https://t.co/UrLvfqaRNL

I'll be tackling modules again. Come along to spectate, heckle, help, or just to have a voice on in the background.","2021-08-26 13:22 +0000","4165.0","128.0","0.030732292917166868","2.0","2.0","12.0","26.0","62.0","0.0","24.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430866928588017665","https://twitter.com/LisaDeBruine/status/1430866928588017665","@sTeamTraen @CFCamerer @stephaniemlee @DataColada Given that the last digit is something that will change the next time you drive it, I can definitely see why people would round, even if they just checked the exact number.","2021-08-26 12:17 +0000","434.0","6.0","0.013824884792626729","0.0","1.0","1.0","1.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430802036119982084","https://twitter.com/LisaDeBruine/status/1430802036119982084","These are absolutely perfect, @djnavarro 😍 I agree with all of the naming conventions espoused here and it’s exactly the right level for the type of students I teach. This is definitely getting linked to the week 1 syllabus. Thanks for sharing! https://t.co/wvaO7xoqgy","2021-08-26 07:59 +0000","4725.0","51.0","0.010793650793650795","2.0","1.0","20.0","6.0","0.0","0.0","22.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430618027188494340","https://twitter.com/LisaDeBruine/status/1430618027188494340","This is on now and it’s really good! https://t.co/a2WXKhASr7","2021-08-25 19:48 +0000","3005.0","17.0","0.005657237936772047","0.0","1.0","5.0","8.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430567100851032066","https://twitter.com/LisaDeBruine/status/1430567100851032066","I am loving the bs4 #bookdown style. I've updated #psyTeachR Data Skills for Reproducible Research and modded the style slightly for more rainbows.
❤️🧡💛💚💙💜
https://t.co/cshcQnetrj","2021-08-25 16:25 +0000","3066.0","131.0","0.04272667971298108","5.0","2.0","29.0","10.0","67.0","0.0","18.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430539443958657029","https://twitter.com/LisaDeBruine/status/1430539443958657029","@bmwiernik Just the table of contents on this is hilarious.","2021-08-25 14:35 +0000","1122.0","15.0","0.013368983957219251","0.0","1.0","7.0","2.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430537321582141454","https://twitter.com/LisaDeBruine/status/1430537321582141454","Chloe maintains a detailed spreadsheet of hundreds of films that feature queer women (and also has a great tiktok account reviewing the films). I made a thing using #shiny to search the spreadsheet more easily. 
https://t.co/g2x0fdqJxl 🏳️‍🌈🏳️‍⚧️

https://t.co/LrzZ2RNFYE","2021-08-25 14:27 +0000","2050.0","38.0","0.018536585365853658","1.0","0.0","2.0","1.0","23.0","0.0","11.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430531068545966094","https://twitter.com/LisaDeBruine/status/1430531068545966094","@WillBall12 @workforcesoton @dsquintana This is a pretty accessible intro to equivalence tests (and why that’s not the same as “not significantly different”) https://t.co/WjpOweADjq","2021-08-25 14:02 +0000","113.0","14.0","0.12389380530973451","0.0","1.0","1.0","1.0","11.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430511717159772161","https://twitter.com/LisaDeBruine/status/1430511717159772161","@JamesEBartlett @UofGPsychology @UofGlasgow Congrats and welcome! It will be really great to have you on the team :)","2021-08-25 12:45 +0000","522.0","5.0","0.009578544061302681","0.0","1.0","1.0","3.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430466093626318850","https://twitter.com/LisaDeBruine/status/1430466093626318850","@kierisi @jsonbecker You're right that ""don't worry if you don't understand"" is an annoying shortcut to communicating the above, and your thread will now prompt me to give a clearer explanation next time I want to say/write that.","2021-08-25 09:44 +0000","131.0","1.0","0.007633587786259542","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430465692894208003","https://twitter.com/LisaDeBruine/status/1430465692894208003","@kierisi @jsonbecker I also find often that I tell learners to make a choice where it isn't obvious yet why one option is better than the other, and tell them that they will understand later why, but the explanation right now is a distraction from the current task.","2021-08-25 09:42 +0000","157.0","4.0","0.025477707006369428","0.0","1.0","0.0","1.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430464894093176832","https://twitter.com/LisaDeBruine/status/1430464894093176832","@kierisi @jsonbecker For example, I like to teach visualisation before data wrangling or even import. But it's better to use data that are relevant to the students, rather than built-in data, so I give them the read_csv code and tell them we'll learn the details about that next week.","2021-08-25 09:39 +0000","113.0","5.0","0.04424778761061947","0.0","1.0","1.0","1.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430464421881593857","https://twitter.com/LisaDeBruine/status/1430464421881593857","@kierisi @jsonbecker In practice I find there's a tradeoff between realistic examples that learners connect to, but which often require an aspect we haven't learned yet, versus simplified examples that only use skills we've learned.","2021-08-25 09:37 +0000","108.0","4.0","0.037037037037037035","0.0","1.0","2.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430157639661625348","https://twitter.com/LisaDeBruine/status/1430157639661625348","@sTeamTraen @RogertheGS @JohnSakaluk @hardsci @ceptional @DrMLChivers “Frat house attitude” is the exact right way to describe this type of work. Whereas real sex researchers are usually so careful and considered. (Although I’m not trying to “no true Scotsman” here; some have been awful, like JM Bailey).","2021-08-24 13:18 +0000","456.0","18.0","0.039473684210526314","0.0","0.0","2.0","1.0","0.0","0.0","15.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1430073367135129601","https://twitter.com/LisaDeBruine/status/1430073367135129601","@AdamRutherford @royalsociety Congratulations and well-deserved!","2021-08-24 07:43 +0000","525.0","1.0","0.0019047619047619048","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429912637907865603","https://twitter.com/LisaDeBruine/status/1429912637907865603","@sTeamTraen @hardsci @ceptional A study being about sex toys doesn’t make it necessarily creepy. Many people do this kind of research well, like @DrMLChivers. My concern is the quirky behavioural econ approach and the clear lack of a respectful and knowledgable approach to the field of sex research.","2021-08-23 21:05 +0000","1270.0","95.0","0.07480314960629922","0.0","3.0","27.0","32.0","0.0","0.0","33.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429910535131344899","https://twitter.com/LisaDeBruine/status/1429910535131344899","@ceptional What do you make of the @R__INDEX chart that Ariely cites at the end to demonstrate the accusations against his work are unfounded?","2021-08-23 20:56 +0000","2094.0","109.0","0.05205348615090735","0.0","2.0","2.0","20.0","0.0","0.0","75.0","0.0","0","0","0","0","0","10","10","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429900423402491906","https://twitter.com/LisaDeBruine/status/1429900423402491906","@DrEliSheff @bondingproject_ FYI: It’s a long quiz and they won’t give you your results unless you give them your email address. The questions are basically at list of facets of relationship anarchy and asking whether you want or don’t want that. You can guess your score easily.","2021-08-23 20:16 +0000","580.0","10.0","0.017241379310344827","0.0","2.0","1.0","3.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429843575186104326","https://twitter.com/LisaDeBruine/status/1429843575186104326","@MaksimShcheglov No thanks!","2021-08-23 16:30 +0000","834.0","65.0","0.07793764988009592","0.0","1.0","11.0","19.0","0.0","0.0","34.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429648600208973828","https://twitter.com/LisaDeBruine/status/1429648600208973828","The PDF version is still giving me headaches after 3 hours of debug, but I'll sort it eventually. I haven't read through the ebook versions carefully yet; they're likely to need some formatting.

FYI - bookdown::epub_book requires you close &lt;img /&gt; tags or it glitches like mad.","2021-08-23 03:36 +0000","1438.0","8.0","0.005563282336578581","0.0","0.0","2.0","4.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429648598065729545","https://twitter.com/LisaDeBruine/status/1429648598065729545","I just posted a draft of the second edition of #psyTeachR Data Skills for Reproducible Research. The order of chapters is rearranged, some sections have been updated, and there are mobi and epub versions. Debugging comments always welcome!

https://t.co/cshcQnetrj","2021-08-23 03:36 +0000","13602.0","312.0","0.02293780326422585","18.0","1.0","78.0","30.0","115.0","2.0","68.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429484870544265217","https://twitter.com/LisaDeBruine/status/1429484870544265217","@usefulButUncool Aye, but my I run my rendering code manually and was *always* forgetting to return the wd to the right directory to do package dev things after.

I could write a script handling it with on.exit(), but the whole thing takes 10 minutes to render and I usually only need parts.","2021-08-22 16:45 +0000","51.0","3.0","0.058823529411764705","0.0","1.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429483743883022340","https://twitter.com/LisaDeBruine/status/1429483743883022340","@_chenson__ Good tip! I do this for the ePub and PDF versions (I really dislike the asymmetric margins), but I don’t mind the default width for web books.","2021-08-22 16:40 +0000","41.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429438358422499330","https://twitter.com/LisaDeBruine/status/1429438358422499330","TIL that if you want to keep all your #rstats #bookdown files in a dir that isn't your project working directory (e.g., when making a package that contains a book), you don't have to change wd to render if you use this code:

xfun::in_dir(""book"", bookdown::render_book())","2021-08-22 13:40 +0000","3746.0","49.0","0.013080619327282435","3.0","2.0","9.0","5.0","0.0","0.0","30.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429411600579141641","https://twitter.com/LisaDeBruine/status/1429411600579141641","@MetaMethodsPH @sTeamTraen @BorsboomDenny @jamesheathers If the SDs are actually SEs, the raw data could look something like this for the questions with smaller vs larger ""SD""s (below are sex with animals versus hate sex). This doesn't look impossible to me, but requires a few very high responses for the clearly illegal/immoral Qs 🤢 https://t.co/rwPpMN9h5c","2021-08-22 11:54 +0000","353.0","36.0","0.10198300283286119","0.0","0.0","4.0","4.0","0.0","0.0","17.0","0.0","0","0","0","0","0","11","11","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429410352035401728","https://twitter.com/LisaDeBruine/status/1429410352035401728","@MetaMethodsPH @sTeamTraen @BorsboomDenny @jamesheathers Those SDs are irrelevant to the p-values. The SD used in a paired comparison is the SD of the difference score. The variation between people can be enormous, but the variation of difference scores can be very small.","2021-08-22 11:49 +0000","282.0","8.0","0.028368794326241134","0.0","1.0","2.0","1.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429406174731030528","https://twitter.com/LisaDeBruine/status/1429406174731030528","@BorsboomDenny @MetaMethodsPH @jamesheathers @sTeamTraen Agreed. One detail -- The N should be 24 per condition (the two groups of 12 did the non and arounsed conditions in counterbalanced order, and one group did the non condition twice and their scores were averaged).","2021-08-22 11:32 +0000","153.0","2.0","0.013071895424836602","0.0","0.0","0.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429386909760040961","https://twitter.com/LisaDeBruine/status/1429386909760040961","@EvoCourtney @MetaMethodsPH No rush! Enjoy your weekend.","2021-08-22 10:16 +0000","123.0","4.0","0.032520325203252036","0.0","0.0","1.0","2.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429132993503928322","https://twitter.com/LisaDeBruine/status/1429132993503928322","@EvoCourtney @MetaMethodsPH You’ll see further in that thread I’ve done a bunch of plots looking at how those don’t seem realistic. I was actually wondering what the similar data from *your* study looked like. E.g., what are the individual question means and SDs?","2021-08-21 17:27 +0000","321.0","7.0","0.021806853582554516","0.0","1.0","1.0","2.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429081451442679808","https://twitter.com/LisaDeBruine/status/1429081451442679808","@EvoCourtney @MetaMethodsPH Do you have insight on the SDs reported in the original paper, given you also collected data on a 0-100 scale?","2021-08-21 14:02 +0000","479.0","9.0","0.018789144050104383","0.0","0.0","1.0","5.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429031385541529602","https://twitter.com/LisaDeBruine/status/1429031385541529602","@VerbingNouns Congrats!!","2021-08-21 10:43 +0000","188.0","1.0","0.005319148936170213","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429019115319177220","https://twitter.com/LisaDeBruine/status/1429019115319177220","@mmuthukrishna @NatureHumBehav I’m definitely 100% for code review. Science would improve if all code were checked (and more unit tests!) If journals add any “extra value” to papers, it’s through things like this. But they need to pay experts, not get probably under-qualified reviewers to do it for free.","2021-08-21 09:54 +0000","110.0","2.0","0.01818181818181818","0.0","0.0","1.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429018303578660864","https://twitter.com/LisaDeBruine/status/1429018303578660864","@mmuthukrishna @NatureHumBehav The @RedTeamMarket is one way to hire someone to do code review. It would be good if journals could accept some authenticated external review.","2021-08-21 09:51 +0000","804.0","7.0","0.008706467661691543","1.0","0.0","0.0","5.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429014736146993154","https://twitter.com/LisaDeBruine/status/1429014736146993154","@mmuthukrishna @NatureHumBehav Although this is another argument for #TeamScience. People who write the main code for projects should spend time developing skills for clarity, but not every member of a team needs to develop that particular expertise.","2021-08-21 09:37 +0000","206.0","7.0","0.03398058252427184","0.0","1.0","5.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429014094326161411","https://twitter.com/LisaDeBruine/status/1429014094326161411","@mmuthukrishna @NatureHumBehav It’s taken several years of sharing and reading code for me to understand how to structure it so someone else can easily review it. I still have lots of room for improvement, but some of my early code is impenetrable. Researchers need to take coding CPD seriously.","2021-08-21 09:34 +0000","261.0","17.0","0.06513409961685823","0.0","2.0","7.0","3.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429011007092019200","https://twitter.com/LisaDeBruine/status/1429011007092019200","@mmuthukrishna @NatureHumBehav I’d be happier to do this if I’m *just* a code reviewer and don’t have to provide full review of the other aspects of the manuscript. Otherwise, the process will take 3-5 times longer than a review without code.","2021-08-21 09:22 +0000","499.0","19.0","0.03807615230460922","0.0","2.0","8.0","1.0","0.0","0.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1429010717324320771","https://twitter.com/LisaDeBruine/status/1429010717324320771","@mmuthukrishna @NatureHumBehav I have the skills for this (on some topics/languages) but it takes a *lot* of time to do properly. A simple check that code runs is one thing, but just checking all the reported numbers is time consuming unless the authors are very organised, much less checking for coding logic.","2021-08-21 09:21 +0000","2577.0","45.0","0.017462165308498253","1.0","1.0","15.0","1.0","0.0","0.0","27.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428999732609884160","https://twitter.com/LisaDeBruine/status/1428999732609884160","@hansijzerman @PoliPsyProf @ylelkes The main suggestion in that thread is that SEMs may have been mistakenly reported instead of SDs. If the plots investigating that led you to a conclusion of fraud… we’ll there’s certainly not enough info in the paper to conclude that.","2021-08-21 08:37 +0000","413.0","14.0","0.03389830508474576","0.0","0.0","2.0","2.0","0.0","0.0","10.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428766372348190735","https://twitter.com/LisaDeBruine/status/1428766372348190735","@araami1 I’m not sure what you’re asking there.","2021-08-20 17:10 +0000","42.0","2.0","0.047619047619047616","0.0","0.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428741264019701763","https://twitter.com/LisaDeBruine/status/1428741264019701763","@sTeamTraen @MetaMethodsPH @sharoz @MaartenvSmeden @EikoFried @jamesheathers Ariely says on his website he doesn't respond to emails, so I'll email Loewenstein's administrative assistant (he doesn't have a listed email) and CC the JBDM editor for info. I expect zero response, but here goes.","2021-08-20 15:30 +0000","1077.0","66.0","0.06128133704735376","0.0","3.0","3.0","6.0","1.0","0.0","52.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428736871178379264","https://twitter.com/LisaDeBruine/status/1428736871178379264","@sTeamTraen @MetaMethodsPH @sharoz @MaartenvSmeden @EikoFried @jamesheathers I'd be happy to ask. The journal currently has a good policy on data sharing, but I expect data from a 2006 paper to have been lost or destroyed after 10 years based on older policies.
https://t.co/M5aLKoRWyS","2021-08-20 15:13 +0000","564.0","22.0","0.03900709219858156","0.0","1.0","5.0","5.0","3.0","0.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428728156329070595","https://twitter.com/LisaDeBruine/status/1428728156329070595","@sharoz @sTeamTraen @MaartenvSmeden @EikoFried @MetaMethodsPH @jamesheathers Has anyone asked for the raw data yet?","2021-08-20 14:38 +0000","499.0","17.0","0.03406813627254509","0.0","2.0","1.0","2.0","0.0","0.0","12.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428727972769435661","https://twitter.com/LisaDeBruine/status/1428727972769435661","@sharoz @sTeamTraen @MaartenvSmeden @EikoFried @MetaMethodsPH @jamesheathers I really feel like that's not enough 0s unless people are way more awful than I thought. I have a hard time studying how kin recognition influences incest avoidance because almost nobody will (admit to) attraction to a sibling above the floor value.","2021-08-20 14:37 +0000","141.0","9.0","0.06382978723404255","0.0","1.0","1.0","0.0","0.0","0.0","5.0","0.0","0","0","0","0","0","2","2","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428726210754920448","https://twitter.com/LisaDeBruine/status/1428726210754920448","@sTeamTraen @sharoz @MaartenvSmeden @EikoFried @MetaMethodsPH @jamesheathers For non-arousal condition, the granularity is double that because half of the subjects' scores are averages of two 0-25 scores.","2021-08-20 14:30 +0000","338.0","11.0","0.03254437869822485","0.0","1.0","1.0","0.0","0.0","0.0","6.0","0.0","0","0","0","0","0","3","3","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428714697495400452","https://twitter.com/LisaDeBruine/status/1428714697495400452","@MetaMethodsPH @jamesheathers @sTeamTraen I'm taking this back. The low SDs for the very low means doesn't make sense. If there were lots of 0 values and the means were driven by a few outliers, this should make their SDs much higher. These SDS are consistent with most values really near the non-zero mean.","2021-08-20 13:45 +0000","82.0","6.0","0.07317073170731707","0.0","0.0","2.0","1.0","0.0","0.0","2.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428712417983336453","https://twitter.com/LisaDeBruine/status/1428712417983336453","@sTeamTraen @MetaMethodsPH @RogertheGS @jamesheathers But the fact that the original responses were 0-26, half the responses are averages of two sessions, and the means are these values multiplied by 4 and rounded to the nearest integer, makes SPRITE pretty difficult to use conclusively.","2021-08-20 13:35 +0000","147.0","5.0","0.034013605442176874","0.0","1.0","2.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428711770491965450","https://twitter.com/LisaDeBruine/status/1428711770491965450","@sTeamTraen @MetaMethodsPH @RogertheGS @jamesheathers Probably the most tractable (and disturbing) stat is in response to the question about willingness to drug a woman. The non-aroused mean is 5 and SD = 2.51. You can't get an SD that low with mainly 0s and a few outliers. I think most answers would have to be low but non-0.","2021-08-20 13:33 +0000","145.0","3.0","0.020689655172413793","0.0","1.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428709045368668160","https://twitter.com/LisaDeBruine/status/1428709045368668160","@RogertheGS @sTeamTraen @MetaMethodsPH @jamesheathers So many things about this study are WTF. If you're going to use a non-representative image anyway, why on earth choose that one? We all know what you mean by porn. The bit in the book about shopping for a department whose ethics would let them do this is so dodgy, too.","2021-08-20 13:22 +0000","853.0","46.0","0.053927315357561546","0.0","2.0","5.0","3.0","0.0","0.0","32.0","0.0","0","0","0","0","0","4","4","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428704710094114826","https://twitter.com/LisaDeBruine/status/1428704710094114826","@sTeamTraen @MetaMethodsPH @RogertheGS @jamesheathers Half of the subjects' non-aroused responses are averages of two non-aroused sessions that are 1 day before and 1 day after the aroused session. They justify this with a bunch of non-significant p-values in ANOVAs looking at between-subject comparisons.","2021-08-20 13:05 +0000","767.0","26.0","0.03389830508474576","0.0","2.0","2.0","2.0","0.0","0.0","19.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428703694124691459","https://twitter.com/LisaDeBruine/status/1428703694124691459","@sTeamTraen @MetaMethodsPH @RogertheGS @jamesheathers And the design isn't as straightforward as I'd thought. https://t.co/i3xjUTuTMN","2021-08-20 13:01 +0000","784.0","69.0","0.08801020408163265","0.0","1.0","3.0","2.0","0.0","0.0","3.0","0.0","0","0","0","0","0","60","60","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428703169874472961","https://twitter.com/LisaDeBruine/status/1428703169874472961","@sTeamTraen @MetaMethodsPH @RogertheGS @jamesheathers I didn't catch that!

""Because the movement of the probe on the visual-analog scale was carried out by repeatedly pressing keys, we used a discrete scale with twenty-six steps along the visual-analog scale. Responses were converted to a 0–100 scale...""","2021-08-20 12:59 +0000","681.0","24.0","0.03524229074889868","0.0","2.0","3.0","0.0","0.0","0.0","18.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428702292493090822","https://twitter.com/LisaDeBruine/status/1428702292493090822","@bmwiernik @sTeamTraen @MetaMethodsPH @RogertheGS @jamesheathers I’ve been trying to write the code for that, but the p-values are frustratingly imprecise, and lots &lt;.001, so you’d recover a wide range of possible SDs for each test.","2021-08-20 12:55 +0000","155.0","7.0","0.04516129032258064","0.0","1.0","1.0","0.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428697237421535234","https://twitter.com/LisaDeBruine/status/1428697237421535234","@MetaMethodsPH @RogertheGS @jamesheathers @sTeamTraen But you can't recover the SD of the difference scores, which is what would have been used in the paired tests, right?","2021-08-20 12:35 +0000","595.0","15.0","0.025210084033613446","0.0","1.0","4.0","2.0","0.0","0.0","7.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428696562067288069","https://twitter.com/LisaDeBruine/status/1428696562067288069","@tegladwintmp2 @MetaMethodsPH That was the first one I did :)
https://t.co/uyXb2z1MlU","2021-08-20 12:32 +0000","92.0","6.0","0.06521739130434782","0.0","1.0","1.0","0.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428696157048524801","https://twitter.com/LisaDeBruine/status/1428696157048524801","@sharoz @MaartenvSmeden @EikoFried @MetaMethodsPH @jamesheathers @sTeamTraen The plot on the right looks more realistic, and it's a paired design, so the SDs of each paired value tell us basically nothing about the SD of the difference score (which is semi-recoverable from the p-values).","2021-08-20 12:31 +0000","311.0","19.0","0.06109324758842444","0.0","1.0","3.0","1.0","0.0","0.0","11.0","0.0","0","0","0","0","0","3","3","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428694805534674950","https://twitter.com/LisaDeBruine/status/1428694805534674950","Does anyone have experience with visual analogue scales (0-100) and Qs that elicit ceiling/floor effects or bimodal responses? How realistic are these plots? See @MetaMethodsPH's whole thread for more context, analysis &amp; plots, plus my gist for data &amp; code
https://t.co/XX4U1HG83A https://t.co/46HmTXyZQD","2021-08-20 12:26 +0000","2969.0","46.0","0.015493432132030987","0.0","1.0","3.0","6.0","6.0","0.0","30.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428692977547948033","https://twitter.com/LisaDeBruine/status/1428692977547948033","@MetaMethodsPH @jamesheathers @sTeamTraen What do people make of this plot of the non- vs aroused means and SD correlations. https://t.co/B1w4T0BjUz","2021-08-20 12:18 +0000","3243.0","124.0","0.038236201048411966","1.0","1.0","1.0","1.0","0.0","0.0","42.0","0.0","0","0","0","0","0","78","78","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428691050919497739","https://twitter.com/LisaDeBruine/status/1428691050919497739","@MaartenvSmeden @EikoFried @MetaMethodsPH @jamesheathers @sTeamTraen Aye, but as Miguel pointed out, the p-values are consistent with those actually being SDs. The distributions will definitely either be massively skewed, 0-inflated, and/or bimodal, so it's pretty hard to guess at the raw data.
https://t.co/ADZCrv1Pym","2021-08-20 12:11 +0000","602.0","29.0","0.04817275747508306","0.0","1.0","5.0","1.0","1.0","0.0","21.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428688353994723331","https://twitter.com/LisaDeBruine/status/1428688353994723331","@MetaMethodsPH @jamesheathers @sTeamTraen I assume (and would hope) for a lot of the questions, like sex with an animal, most of the responses are 0 exactly. The distribution for the 12-year-old girl is disturbing, though.","2021-08-20 12:00 +0000","545.0","13.0","0.023853211009174313","0.0","1.0","2.0","6.0","0.0","0.0","3.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428687539095932934","https://twitter.com/LisaDeBruine/status/1428687539095932934","@MetaMethodsPH @jamesheathers @sTeamTraen Here's the plot of mean vs SD. I'm trying to think what this ""should"" look like for a visual-analog scale. It makes sense that there are smaller SDs for lower means. The very low SD for the mean around 75 is weird, though. https://t.co/xxkE6hh8Qg","2021-08-20 11:57 +0000","240.0","26.0","0.10833333333333334","1.0","1.0","1.0","0.0","0.0","0.0","9.0","0.0","0","0","0","0","0","14","14","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428686322726805513","https://twitter.com/LisaDeBruine/status/1428686322726805513","@MetaMethodsPH @jamesheathers @sTeamTraen With only 24 values and several of these questions being likely to elicit a bimodal distribution, this definitely isn't a simulation of what the raw data would have looked like, but helps me visualise what's going on better than the table of means and SDs.","2021-08-20 11:52 +0000","422.0","9.0","0.02132701421800948","0.0","1.0","2.0","4.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428684588608598019","https://twitter.com/LisaDeBruine/status/1428684588608598019","@MetaMethodsPH @jamesheathers @sTeamTraen And if those are SEs, here is the simulation. https://t.co/Y9jIwBrv9y","2021-08-20 11:45 +0000","331.0","65.0","0.19637462235649547","0.0","2.0","3.0","1.0","0.0","0.0","12.0","0.0","0","0","0","0","0","47","47","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428684475383300100","https://twitter.com/LisaDeBruine/status/1428684475383300100","@MetaMethodsPH @jamesheathers @sTeamTraen If those are SD values, here is a simulation of the possible distributions (using rtruncnorm to sim values with those means and SDs limited 0 to 100) https://t.co/ZZqTUBDyVD","2021-08-20 11:44 +0000","309.0","65.0","0.21035598705501618","0.0","1.0","5.0","1.0","0.0","0.0","6.0","0.0","0","0","0","0","0","52","52","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428679966745636868","https://twitter.com/LisaDeBruine/status/1428679966745636868","@MetaMethodsPH @jamesheathers @sTeamTraen In case anyone else wants to play with this, I'm starting a gist where I'll put the code to get the data from those tables (it's a pain to transcribe from the image, so no sense in multiple people doing it).

https://t.co/XX4U1HG83A","2021-08-20 11:27 +0000","563.0","17.0","0.03019538188277087","0.0","1.0","8.0","1.0","5.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428675448783900673","https://twitter.com/LisaDeBruine/status/1428675448783900673","@MetaMethodsPH @jamesheathers @sTeamTraen Are those maybe SEs instead of SDs? The first table caption says SD, but they’re way too narrow for a visual analog scale where you know you’ll get a huge range of answers.","2021-08-20 11:09 +0000","2036.0","60.0","0.029469548133595286","1.0","2.0","5.0","5.0","0.0","0.0","43.0","0.0","0","0","0","0","0","4","4","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428303142974894084","https://twitter.com/LisaDeBruine/status/1428303142974894084","@HyndlandScience @mr_downie @RoyalSocBio https://t.co/eJTaLh7988","2021-08-19 10:29 +0000","472.0","7.0","0.014830508474576272","0.0","0.0","3.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","42","2","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428287869282656256","https://twitter.com/LisaDeBruine/status/1428287869282656256","@CarolynBot This is why I love coding shiny apps on Twitch. Just the fact that someone might be watching makes it less weird and I also explain more, which helps my coding. It's way less stressful than a code tutorial where I have to be prepared and non-chaotic.","2021-08-19 09:28 +0000","756.0","7.0","0.009259259259259259","0.0","1.0","4.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428285415291772939","https://twitter.com/LisaDeBruine/status/1428285415291772939","@AndrewStruan @emilynordmann You also might be interested in this @beautiful_hack  workshop that @UofGPsychology #psyTeachR  postgrads put on a few years ago for humanities and social sciences students. All the materials are still available.

https://t.co/vnaOEFAKb4","2021-08-19 09:19 +0000","102.0","13.0","0.12745098039215685","0.0","1.0","0.0","1.0","11.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428284845441142784","https://twitter.com/LisaDeBruine/status/1428284845441142784","@AndrewStruan @emilynordmann Definitely. There will only be descriptive stats and no R experience expected. The course is designed for people like admin staff who need to create reports from data and want to do it in a way that is reproducible, less error-prone, saves time, and uses beautiful visualisations.","2021-08-19 09:16 +0000","98.0","6.0","0.061224489795918366","0.0","0.0","4.0","1.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428284157642346496","https://twitter.com/LisaDeBruine/status/1428284157642346496","@wgervais @Moshe_Hoffman Probably our timelines are different, but mine is definitely discussing systemic issues. And more concerned that the existing ""science cops"" are hugely disincentivised than arguing for more. The ones in this case are remaining anonymous because of fear of backlash, right?","2021-08-19 09:14 +0000","260.0","4.0","0.015384615384615385","0.0","0.0","1.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428282099593850881","https://twitter.com/LisaDeBruine/status/1428282099593850881","I got to see this yesterday and it's fantastic! #Prosopagnosia is an intimate portrayal of what it's like to live with face blindness.  @faceblinduk @ProfSarahBate @twf_team https://t.co/bpMJRynahe","2021-08-19 09:06 +0000","2774.0","27.0","0.00973323720259553","5.0","0.0","12.0","6.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428261461151916037","https://twitter.com/LisaDeBruine/status/1428261461151916037","Two PhDs starting in January on a really interesting meta-scientific project with two outstanding supervisors. https://t.co/mKdBHfnLQD","2021-08-19 07:44 +0000","3352.0","20.0","0.0059665871121718375","3.0","0.0","10.0","4.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428113828072591364","https://twitter.com/LisaDeBruine/status/1428113828072591364","@talyarkoni @NoahHaber And I’m just saying if they have a hard time detecting errors that an undergrad should be able to spot after more than a decade of follow-up papers, probably nobody is looking too hard for fraud.","2021-08-18 21:57 +0000","225.0","5.0","0.022222222222222223","0.0","1.0","0.0","0.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428109929383174146","https://twitter.com/LisaDeBruine/status/1428109929383174146","@talyarkoni @NoahHaber I also found this snarky report of an 11-year series of “wrong papers” where “…the mistake in all papers is very simple and at the level of undergraduate students: authors were not able to construct adjoint integral operator correctly.”

https://t.co/XikRTThaeJ","2021-08-18 21:41 +0000","207.0","5.0","0.024154589371980676","0.0","0.0","1.0","0.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428109224660414465","https://twitter.com/LisaDeBruine/status/1428109224660414465","@talyarkoni @NoahHaber A mathematician claims 1/3 of published maths papers have an error that wasn’t caught in review. https://t.co/uGQ0Pw2fdx","2021-08-18 21:39 +0000","449.0","29.0","0.0645879732739421","0.0","3.0","4.0","0.0","8.0","0.0","14.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428108184959455234","https://twitter.com/LisaDeBruine/status/1428108184959455234","@talyarkoni @NoahHaber I cannot recall the source right now, but was reading recently about how most mathematic papers have never been verified because there are so few people with all the requisite skills. So it may just be really hard to detect or to distinguish from honest mistakes.","2021-08-18 21:34 +0000","462.0","16.0","0.03463203463203463","0.0","2.0","3.0","0.0","1.0","0.0","10.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428104770829565955","https://twitter.com/LisaDeBruine/status/1428104770829565955","@EikoFried @MarcusCrede I’ve got a walkthrough of analyses from a very cool @Meta_Psy paper that I use to teach simulation that shows how to calculate this.

https://t.co/xru31f0Grj","2021-08-18 21:21 +0000","531.0","36.0","0.06779661016949153","0.0","0.0","8.0","2.0","22.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428102067932082176","https://twitter.com/LisaDeBruine/status/1428102067932082176","@sTeamTraen Copy editors! I spent a few minutes googling “what’s the opposite of an action editor” and just could not remember the right term :)","2021-08-18 21:10 +0000","398.0","12.0","0.03015075376884422","0.0","1.0","0.0","1.0","0.0","0.0","8.0","0.0","0","0","0","0","0","2","2","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428099361414815746","https://twitter.com/LisaDeBruine/status/1428099361414815746","@sTeamTraen Having seen how often writing editors at scientific journals mangle the meaning when trying to “fix” grammar/writing, I’d recommend erring on the side of copying when reporting factual details. This sort of plagiarism rates about a 0.2/10 on my scale of things to worry about.","2021-08-18 20:59 +0000","205.0","3.0","0.014634146341463415","0.0","1.0","1.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428098398826209282","https://twitter.com/LisaDeBruine/status/1428098398826209282","@talyarkoni @NoahHaber Not theoretical, but:
https://t.co/uVbCe18DO1","2021-08-18 20:56 +0000","412.0","16.0","0.038834951456310676","0.0","1.0","2.0","0.0","9.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428095666157408258","https://twitter.com/LisaDeBruine/status/1428095666157408258","@sTeamTraen I’d really like to see a dozen academics try to summarise that study without using most of those phrases. I’m not quite sure who it’s hurting to describe a study using much of the original wording. It at least avoids a game of research literature telephone.","2021-08-18 20:45 +0000","309.0","34.0","0.11003236245954692","0.0","2.0","3.0","2.0","0.0","0.0","20.0","0.0","0","0","0","0","0","7","7","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428034283839574019","https://twitter.com/LisaDeBruine/status/1428034283839574019","@JustinWolfers The only thing that I think could possibly support the idea that an insurance company employee faked those data is the use of a uniform distribution. I’d hope a social scientist would know better. But how would the company know to fake data int he right direction?","2021-08-18 16:41 +0000","708.0","11.0","0.015536723163841809","0.0","1.0","3.0","2.0","1.0","0.0","3.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1428013623046254595","https://twitter.com/LisaDeBruine/status/1428013623046254595","@NoahHaber @KLdivergence @Corey_Yanofsky I’m definitely overestimating how much it would bug the average scientist to fake data so stupidly.

But yeah, the argument for it being faked by the insurance company is incredibly weak and the uniform distribution is the *only* support I can see. The H1 consistency is very sus.","2021-08-18 15:19 +0000","115.0","10.0","0.08695652173913043","0.0","0.0","3.0","0.0","0.0","0.0","7.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427946506267598852","https://twitter.com/LisaDeBruine/status/1427946506267598852","@VandekerckhoveJ @Lincoln81 @rlmcelreath I can’t think of anything that wouldn’t either be easy to get around by rounding or adding tiny random values, or would make it less useful for things like power analyses by simulation.","2021-08-18 10:52 +0000","638.0","13.0","0.02037617554858934","0.0","3.0","1.0","0.0","0.0","0.0","9.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427908761423863811","https://twitter.com/LisaDeBruine/status/1427908761423863811","@NoahHaber @Corey_Yanofsky @KLdivergence The uniform distribution is the one thing that makes me think it could have been someone from the car company trying to cover up that they didn't do what they said they'd do. I'd like to think the scientists would understand distributions slightly better than that.","2021-08-18 08:22 +0000","183.0","8.0","0.04371584699453552","0.0","1.0","5.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427903019937673221","https://twitter.com/LisaDeBruine/status/1427903019937673221","@ingorohlfing @rlmcelreath Me too. I think learning simulation earlier could have prevented so many stupid analyses in my past work. The videos at https://t.co/adStpUGAEU by @sim_school are a good place to start if you're interested in data simulation.","2021-08-18 07:59 +0000","1734.0","96.0","0.05536332179930796","4.0","1.0","10.0","16.0","56.0","0.0","9.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427902371070423046","https://twitter.com/LisaDeBruine/status/1427902371070423046","@JeffRouder # with a regression line
ggplot(df_wide, aes(x = Q1, y = Q2, )) +
  geom_point() +
  geom_smooth(method = lm) https://t.co/lHWj4cd4dH","2021-08-18 07:57 +0000","206.0","10.0","0.04854368932038835","0.0","0.0","3.0","0.0","0.0","0.0","5.0","0.0","0","0","0","0","0","2","2","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427901775152099333","https://twitter.com/LisaDeBruine/status/1427901775152099333","@JeffRouder # filter and make wide
df_wide &lt;- df %&gt;%
  filter(question %in% c(""Q1"", ""Q2"")) %&gt;%
  pivot_wider(names_from = question,
              values_from = score)

# scatterplot
ggplot(df_wide, aes(x = Q1, y = Q2)) +
  geom_point()","2021-08-18 07:54 +0000","247.0","9.0","0.03643724696356275","0.0","1.0","4.0","1.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427901666574098438","https://twitter.com/LisaDeBruine/status/1427901666574098438","@JeffRouder library(tidyverse)

# simulate data
df &lt;- crossing(
  class = 1:81, 
  question = paste0(""Q"", 1:18)) %&gt;%
  mutate(score = rnorm(nrow(.)))","2021-08-18 07:54 +0000","155.0","4.0","0.025806451612903226","0.0","1.0","1.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427899905276788740","https://twitter.com/LisaDeBruine/status/1427899905276788740","@rlmcelreath I really do worry that providing tools like faux is going to help people commit fraud. But the benefits of helping more people do data simulation for legitimate purposes outweighs that, I think.","2021-08-18 07:47 +0000","4016.0","104.0","0.025896414342629483","0.0","3.0","30.0","13.0","3.0","0.0","55.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427899254660599808","https://twitter.com/LisaDeBruine/status/1427899254660599808","@JeffRouder The pivot functions take a little bit of conceptual rethinking, but are really powerful once you’ve sorted that. 

https://t.co/NzFx9wVLKD","2021-08-18 07:44 +0000","216.0","12.0","0.05555555555555555","0.0","0.0","2.0","1.0","7.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427898827311304704","https://twitter.com/LisaDeBruine/status/1427898827311304704","@JeffRouder All our #psyTeachR books use tidyverse. Probably the quickest dplyr/tidyr/ggplot intro would be in the MSc Data Skills book.

And I’m totally happy to take specific questions. I love tidyverse for analyses and base for package development. 

https://t.co/FTajzu4LNb","2021-08-18 07:43 +0000","1477.0","111.0","0.07515233581584292","3.0","0.0","26.0","11.0","59.0","4.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427660542290939909","https://twitter.com/LisaDeBruine/status/1427660542290939909","@NoahHaber I guess if you'd just plotted the baseline and update values on their own, they'd both look reasonably distributed. It was only the difference between them that had a uniform distribution, which could be easily missed even if you plot raw data.","2021-08-17 15:56 +0000","112.0","10.0","0.08928571428571429","0.0","1.0","1.0","0.0","0.0","0.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427659831264096265","https://twitter.com/LisaDeBruine/status/1427659831264096265","@economeager @NoahHaber The full post just gets more and more convincing. The section on rows in Calibri vs Cambria font is amazing. There is zero chance that the CDFs for two random halves of the data would match like that. https://t.co/Sp4QaTTeVT","2021-08-17 15:53 +0000","436.0","99.0","0.22706422018348624","0.0","0.0","5.0","4.0","0.0","0.0","8.0","0.0","0","0","0","0","0","82","82","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427655990581866496","https://twitter.com/LisaDeBruine/status/1427655990581866496","This is why you always plot your raw data. https://t.co/effWQNqzRu","2021-08-17 15:38 +0000","7112.0","153.0","0.021512935883014624","7.0","0.0","37.0","24.0","1.0","0.0","84.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427591517233942536","https://twitter.com/LisaDeBruine/status/1427591517233942536","@cantabile Last week I found out that the twitch streamers (except for @_TanHo) love {golem} for packaging up shiny apps. I’m not ready to add that to my workflow, but YMMV.","2021-08-17 11:21 +0000","114.0","3.0","0.02631578947368421","0.0","0.0","3.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427585408653742088","https://twitter.com/LisaDeBruine/status/1427585408653742088","I’ll be working on a #shiny app for data simulation using faux on Twitch in an hour. Today I think I’ll restructure the factor input to use modules, add inputs for custom factor and level names, and start exploring shinyWidgets. 

https://t.co/UrLvfqaRNL","2021-08-17 10:57 +0000","1532.0","11.0","0.007180156657963447","0.0","0.0","1.0","2.0","3.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427584579343470607","https://twitter.com/LisaDeBruine/status/1427584579343470607","@cantabile I’ll be adding modules to the faux app today at 1:00!","2021-08-17 10:54 +0000","508.0","8.0","0.015748031496062992","0.0","1.0","1.0","3.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427319130735222787","https://twitter.com/LisaDeBruine/status/1427319130735222787","So I am asking for opinions here. Is this a type of project that is poisoned from the start? Is there a way to visualize human diversity using face composites that doesn't reinforce prejudice? Are there issues I haven't thought of?","2021-08-16 17:19 +0000","3453.0","140.0","0.040544454097885894","3.0","1.0","10.0","4.0","0.0","0.0","116.0","0.0","0","0","0","0","0","6","6","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427319127950217219","https://twitter.com/LisaDeBruine/status/1427319127950217219","And that I wouldn't be involved in a project that reinforces a gender binary or an essentialized view of ethnicity. But I'm not sure there is any way to categorize people that doesn't do that. Even categorizing by country will get people policing the ""color"" of the composite.","2021-08-16 17:19 +0000","2332.0","45.0","0.019296740994854202","1.0","3.0","15.0","4.0","0.0","0.0","22.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427319124288577538","https://twitter.com/LisaDeBruine/status/1427319124288577538","I've told them I can't be involved unless the people depicted in the images going into the composite face have given explicit permission to be used in this project or are part of an open source image set (where the terms of use are compatible).","2021-08-16 17:19 +0000","974.0","22.0","0.022587268993839837","0.0","1.0","12.0","2.0","0.0","0.0","7.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427319120815665158","https://twitter.com/LisaDeBruine/status/1427319120815665158","As happens every few months, I've been contacted by someone who wants my help visualizing all the humans. While I have the technical know-how, I'm more and more concerned by the ethical implications. (🧵...) https://t.co/id0PX9B4DS","2021-08-16 17:19 +0000","6995.0","680.0","0.09721229449606862","5.0","1.0","18.0","30.0","1.0","0.0","387.0","0.0","0","0","0","0","0","238","238","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427273969686745094","https://twitter.com/LisaDeBruine/status/1427273969686745094","@Hao_and_Y But just FYI, it's

Remotes:  
    psyteachr/psyteachr","2021-08-16 14:20 +0000","61.0","2.0","0.03278688524590164","0.0","0.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427273463211909123","https://twitter.com/LisaDeBruine/status/1427273463211909123","@Hao_and_Y Oh, of course there's a usethis function for it!! I was thinking it would be an option to usethis::use_package().","2021-08-16 14:18 +0000","83.0","1.0","0.012048192771084338","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427268038898900993","https://twitter.com/LisaDeBruine/status/1427268038898900993","How do you Import or Suggest a non-CRAN package in the DESCRIPTIONS file of an #rstats package?

I can never remember how to do this when I need to (I've already checked https://t.co/jTrjoZ664P) and  I can't find any examples in my own files since faux got on CRAN.","2021-08-16 13:56 +0000","3208.0","44.0","0.01371571072319202","1.0","2.0","1.0","2.0","1.0","0.0","37.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427184379059654659","https://twitter.com/LisaDeBruine/status/1427184379059654659","@Limor_Raviv I am Lisa, Goddess of the time you have to be an adult. Fear me because I’m tired. 🤣","2021-08-16 08:24 +0000","579.0","4.0","0.0069084628670120895","0.0","0.0","2.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427183219741442048","https://twitter.com/LisaDeBruine/status/1427183219741442048","I totally forgot we have a psyTeachR work day Thursday, so I've moved my twitch stream for this app to Tuesday at 13:00 GMT+1. I got IT to install shinyWidgets on our shiny server, so you can watch me explore that.

https://t.co/fDBoLogAra #shiny #rstats #simulation https://t.co/tASME3rNqv","2021-08-16 08:19 +0000","3222.0","22.0","0.006828057107386716","2.0","0.0","2.0","5.0","6.0","1.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427168728345726980","https://twitter.com/LisaDeBruine/status/1427168728345726980","@dylanwiliam This may be the only good idea that came out of the REF.","2021-08-16 07:21 +0000","72.0","1.0","0.013888888888888888","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427168455267168256","https://twitter.com/LisaDeBruine/status/1427168455267168256","@NauderNamaky @BrianNosek We need to let people publish smaller units of research, like code, software, and associated methods papers, in ways that get appropriate recognition in hiring and promotion. This would de-incentivise methods hoarding for co-authorship.","2021-08-16 07:20 +0000","58.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1427161913453129730","https://twitter.com/LisaDeBruine/status/1427161913453129730","@BrianNosek @annemscheel But only if they took credit for the methods in the contributorship list (which all papers should have). Your suspicion of these “authors” is probably warranted, but we can’t fix it by pushing a lone genius model where the PI has to be an expert at everything. #TeamScience","2021-08-16 06:54 +0000","651.0","11.0","0.016897081413210446","0.0","0.0","6.0","2.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426913245617610757","https://twitter.com/LisaDeBruine/status/1426913245617610757","That was fun again. @_TanHo taught me about shinyWidgets, which are beautiful, but not on the shiny server I use yet, so commented out in the code for this, which is at https://t.co/0eibtI1sQE

The new version has filters for happy endings and bury your gays, plus bug fixes.","2021-08-15 14:26 +0000","1540.0","21.0","0.013636363636363636","0.0","1.0","2.0","1.0","15.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426864526884999170","https://twitter.com/LisaDeBruine/status/1426864526884999170","I have to reward myself for finishing my marking, so I'm going to work on an #rstats #shiny app to search a public list of lesbian films (https://t.co/g2x0fdqJxl) when I finish the next batch. I'll stream it on twitch at 1:30 UK if anyone wants to watch. https://t.co/RwLHRJqLlk","2021-08-15 11:13 +0000","6072.0","172.0","0.028326745718050064","3.0","2.0","17.0","31.0","80.0","2.0","37.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426840304045408258","https://twitter.com/LisaDeBruine/status/1426840304045408258","@EmRstats @kierisi The hardest part was figuring out which category my stream should go under. @kierisi and @_TanHo helped me with that (it’s Science and Technology).

What are you thinking about streaming?","2021-08-15 09:36 +0000","64.0","7.0","0.109375","0.0","1.0","2.0","2.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426839945763823616","https://twitter.com/LisaDeBruine/status/1426839945763823616","@EmRstats @kierisi I did my first one last week and it’s pretty straightforward. Set up a twitch account and download the streaming app. It’s kind of like zoom, and you can set up your screen share for where your head and chat feed go, then press a button to start streaming.","2021-08-15 09:35 +0000","82.0","5.0","0.06097560975609756","0.0","1.0","1.0","1.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426837939724693507","https://twitter.com/LisaDeBruine/status/1426837939724693507","@PWGTennant I’m glad it’s not just me! I wondered why it was suddenly confusing.","2021-08-15 09:27 +0000","508.0","12.0","0.023622047244094488","0.0","0.0","0.0","10.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426834459509338112","https://twitter.com/LisaDeBruine/status/1426834459509338112","@TaymAlsalti @lakens If you’re actually not sure, that’s a really good thing to add! I understand you’re talking about going overboard, but adding a bit of this writing style (that most women know well) makes reviews kinder.","2021-08-15 09:13 +0000","154.0","13.0","0.08441558441558442","0.0","1.0","2.0","2.0","0.0","0.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426590910486351878","https://twitter.com/LisaDeBruine/status/1426590910486351878","@kara_woo Thank you so much for describing this! I do almost exactly the same, but have always wondered if this is really what’s meant by refactoring and if I’m doing things “right”.","2021-08-14 17:05 +0000","204.0","7.0","0.03431372549019608","0.0","0.0","2.0","1.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426584879408484352","https://twitter.com/LisaDeBruine/status/1426584879408484352","@pitigoiflavius @HelixkIinge @dionysisreborn @cassaleigh_ So your restaurant server should be allowed to lick your utensils because most of the time you won’t catch anything bad from it?","2021-08-14 16:41 +0000","21.0","1.0","0.047619047619047616","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426540719796273166","https://twitter.com/LisaDeBruine/status/1426540719796273166","@Leigh2DD @MelanieWdwrth @GidMK It would be nice if it cured the brain worms that make people convinced there's a conspiracy to withhold this off-label treatment for an illness they also think doesn't exist.","2021-08-14 13:46 +0000","306.0","16.0","0.05228758169934641","0.0","0.0","9.0","4.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426517242527862789","https://twitter.com/LisaDeBruine/status/1426517242527862789","@DRMacIver Ooh, that's an important accessibility point! We've learned in the @PsySciAcc that resources vary a lot by region and try to accommodate this by using largely platform-independent web-based data collection, but I hadn't thought about it in the context of learning material.","2021-08-14 12:13 +0000","30.0","1.0","0.03333333333333333","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426515713636937732","https://twitter.com/LisaDeBruine/status/1426515713636937732","@DRMacIver I agree (especially for teaching), but also ugh, Windows 😬","2021-08-14 12:07 +0000","27.0","1.0","0.037037037037037035","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426515039301812225","https://twitter.com/LisaDeBruine/status/1426515039301812225","I'd never heard ""ravioli code"" before. I like it!

I think I'll modularise part of the faux shiny app on twitch next time, even though it's only going to be used twice. Two very complicated things need to work identically, so I think this is a good use case. https://t.co/YQUMt1DqAd","2021-08-14 12:04 +0000","1996.0","15.0","0.00751503006012024","0.0","0.0","3.0","2.0","0.0","0.0","10.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426497882325200898","https://twitter.com/LisaDeBruine/status/1426497882325200898","This is a good start, but an intersectional lens can give more insight to what “major demographic characteristics” should be reported and how they should influence a study’s conclusions.","2021-08-14 10:56 +0000","1271.0","7.0","0.0055074744295830055","0.0","0.0","3.0","4.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426497880970387456","https://twitter.com/LisaDeBruine/status/1426497880970387456","The @APA JARS (Journal Article Reporting Standards) recommends papers:

“Report major demographic characteristics (e.g., age, sex, ethnicity, socioeconomic status) and important topic-specific characteristics (e.g., achievement level in studies of educational interventions).”","2021-08-14 10:56 +0000","1329.0","5.0","0.003762227238525207","0.0","1.0","2.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426497879439515650","https://twitter.com/LisaDeBruine/status/1426497879439515650","I think there’s a lot more awareness of this problem in the decade since the Reproducibility Project, such as @LeonidTiokhin’s “Generalizability is not optional” https://t.co/3bxvY8NzLG","2021-08-14 10:56 +0000","380.0","7.0","0.018421052631578946","0.0","1.0","2.0","1.0","1.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426497877786906626","https://twitter.com/LisaDeBruine/status/1426497877786906626","I think this is an important complement to @talyarkoni’s “Generalisability Crisis”. Psychology has a terrible problem considering mostly white, western undergrads to be “unmarked” and only other populations requiring special justification. https://t.co/vEraPIoZNi","2021-08-14 10:56 +0000","9989.0","175.0","0.01751927119831815","6.0","1.0","51.0","19.0","1.0","0.0","97.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426490264051404801","https://twitter.com/LisaDeBruine/status/1426490264051404801","@sharoz How do you reconcile making all functions verbs with the usefulness of starting all related functions with the same prefix for autocomplete? I default to names like get_object, but object_get makes it easier for me as a user to find a function when the verb isn’t obvious.","2021-08-14 10:25 +0000","108.0","3.0","0.027777777777777776","0.0","1.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426487013683998720","https://twitter.com/LisaDeBruine/status/1426487013683998720","@DRMacIver 💯 I'm reading Advanced R and keep finding myself immediately using new conceptual understandings to solve problems that I was probably just flailing at before and randomly changing things until I made the problem go away.","2021-08-14 10:13 +0000","12.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426483053061414913","https://twitter.com/LisaDeBruine/status/1426483053061414913","@DRMacIver Haha, I spent the last week down the rabbit hole of contrast coding (it’s got a horrific jingle-jangle problem) and probably should have asked myself this on Monday.

But at least I wrote documentation so others don’t have to go down the same rabbit hole. https://t.co/zmqRzLjv2q","2021-08-14 09:57 +0000","14.0","1.0","0.07142857142857142","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426481972696735748","https://twitter.com/LisaDeBruine/status/1426481972696735748","I’m excited to attend the first @OSCDelft coffee meet up on reviewing research code to learn more about this.

https://t.co/SuFdFqm0ie https://t.co/FY8eTts1Sk","2021-08-14 09:52 +0000","1704.0","15.0","0.008802816901408451","0.0","0.0","6.0","2.0","3.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426481234499223552","https://twitter.com/LisaDeBruine/status/1426481234499223552","Anyone have useful resources for getting better at this? I spend a lot of time documenting technical things (function help, vignettes, tutorials, #psyTeachR books), and hope I’m OK at it, but can’t easily articulate the principles that differentiate good from bad documentation. https://t.co/IWZkwmx2w4","2021-08-14 09:50 +0000","2412.0","21.0","0.008706467661691543","1.0","1.0","1.0","2.0","0.0","1.0","15.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426479945501626368","https://twitter.com/LisaDeBruine/status/1426479945501626368","An awful lot of this is applicable to the skills you need to be an academic researcher, especially in a field where you need to do a fair amount of coding (which should be anything with data and analyses). https://t.co/gS8Y3QXtQq","2021-08-14 09:44 +0000","2137.0","12.0","0.005615348619560131","0.0","0.0","8.0","0.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426469206472765446","https://twitter.com/LisaDeBruine/status/1426469206472765446","@FraserSneden Yup! I got hooked after a workshop at michigan womyn’s music festival in the late 90s and played in random drum circles for a bit at uni, but haven’t played in years. So now it’s mainly a side table :)","2021-08-14 09:02 +0000","66.0","1.0","0.015151515151515152","0.0","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426228125533450241","https://twitter.com/LisaDeBruine/status/1426228125533450241","https://t.co/rebR6XMztH","2021-08-13 17:04 +0000","3281.0","325.0","0.09905516610789393","1.0","0.0","9.0","4.0","0.0","0.0","12.0","0.0","0","0","0","0","0","299","299","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426203728034607115","https://twitter.com/LisaDeBruine/status/1426203728034607115","All day today I have been standing at my makeshift standing desk, annoyed that I can’t find my glasses. Anyone want to try to beat my “Where’s Waldo?” time of  6 hours? https://t.co/tIrOYSS8yP","2021-08-13 15:27 +0000","6288.0","1023.0","0.16269083969465647","0.0","6.0","33.0","21.0","3.0","0.0","223.0","0.0","0","0","0","0","0","737","737","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426177003837968384","https://twitter.com/LisaDeBruine/status/1426177003837968384","@BrianNosek Has anyone ever written “There is no evidence that X” and *meant* “There is evidence against X”? My impression is that anyone who has actual evidence *against* something (like an equivalence test) uses unambiguous wording.","2021-08-13 13:41 +0000","1375.0","31.0","0.022545454545454546","0.0","2.0","7.0","6.0","0.0","0.0","16.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426144811845881856","https://twitter.com/LisaDeBruine/status/1426144811845881856","@tamarathcke They were face rating studies and many responses were clearly random button-pressing (impossibly fast completion times) or all the same response. Participation was always fully voluntary, so they weren't doing it for money, just mischief.","2021-08-13 11:33 +0000","67.0","2.0","0.029850746268656716","0.0","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1426077204069425153","https://twitter.com/LisaDeBruine/status/1426077204069425153","I always hated when one of the studies on my lab’s experiment site went viral on Reddit because the data were basically unusable. https://t.co/B2JJrCAsXc","2021-08-13 07:04 +0000","4949.0","145.0","0.029298848252172157","1.0","1.0","31.0","15.0","9.0","0.0","88.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425869438503596034","https://twitter.com/LisaDeBruine/status/1425869438503596034","@sharoz Thanks so much to everyone who came! It was my first time using twitch, so I'm not even sure how to check who all came along. There were only 3 names in the chat, so I thought it wasn't many. (The video is saved on my twitch profile page if anyone is interested in the recording.)","2021-08-12 17:19 +0000","115.0","2.0","0.017391304347826087","0.0","1.0","0.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425867654431186944","https://twitter.com/LisaDeBruine/status/1425867654431186944","This was really fun! I'm going to do it again next week, same time. The app I'm working on is at https://t.co/FnS8CNLzCc if you want to check out the code. I put it online at https://t.co/cZYX7j7R6D but there is a bug in the plot that isn't in my local version. https://t.co/opMrUa2cAY","2021-08-12 17:11 +0000","8085.0","50.0","0.006184291898577613","2.0","1.0","7.0","5.0","21.0","0.0","14.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425797844410245120","https://twitter.com/LisaDeBruine/status/1425797844410245120","@abdullah1299 @kierisi -ish. I'm going to work on an app that I want to include in the faux package, so it's not a beginner exercise, but I'll be explaining as I go with beginners in mind.","2021-08-12 12:34 +0000","84.0","2.0","0.023809523809523808","0.0","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425793117425446925","https://twitter.com/LisaDeBruine/status/1425793117425446925","@_TanHo @kierisi Thanks! Those both seem obvious now, but I really did try a lot of searching for relevant categories and tags :)","2021-08-12 12:15 +0000","102.0","4.0","0.0392156862745098","0.0","1.0","1.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425789637419732992","https://twitter.com/LisaDeBruine/status/1425789637419732992","Help, I'm trying to test my twitch stream and it says I need a category to go live! The most relevant category I can find is ""Coding for Carrots"". There is ""Code R"", but it's a racing game, and Shiny is a pinball game. What am I meant to do with this? (tagging @kierisi &amp; @_TanHo)","2021-08-12 12:01 +0000","3401.0","106.0","0.031167303734195826","0.0","1.0","5.0","9.0","0.0","0.0","91.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425763674560598020","https://twitter.com/LisaDeBruine/status/1425763674560598020","@KhandisBlake @AlexandraWorm @AdamBCohen1 Seconding Chicago Faces. But also the London set has 2513 raters’ attractiveness ratings for 102 faces. https://t.co/hvi1RWKSYL","2021-08-12 10:18 +0000","112.0","5.0","0.044642857142857144","0.0","0.0","1.0","0.0","4.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425741028506615809","https://twitter.com/LisaDeBruine/status/1425741028506615809","I’m a little nervous about it. I’m not going into it with a lesson plan or anything; you just get to watch the real-time process of me working on an app. There will be explaining monologue, lots of googling, frustratingly long debugging, and probably some swearing.","2021-08-12 08:48 +0000","1602.0","22.0","0.01373283395755306","0.0","0.0","9.0","4.0","0.0","0.0","9.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425731807824203778","https://twitter.com/LisaDeBruine/status/1425731807824203778","FYI: @cantabile @DrGBuckingham @aftonsteps @ecotrombonegal","2021-08-12 08:12 +0000","1369.0","20.0","0.014609203798392988","0.0","1.0","7.0","5.0","1.0","0.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425730917167943683","https://twitter.com/LisaDeBruine/status/1425730917167943683","I’m going to try my first twitch stream today at 4:30 (UK). I’ll be starting an #rstats #shiny app for simulating factorial data using faux. https://t.co/VYZIjTF0u2","2021-08-12 08:08 +0000","30437.0","398.0","0.013076190163288103","26.0","4.0","74.0","84.0","78.0","7.0","125.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425591364612509703","https://twitter.com/LisaDeBruine/status/1425591364612509703","@Lincoln81 @saavikford Same here. I’ll be following this thread to see if someone solves how to explain this to students.","2021-08-11 22:54 +0000","661.0","2.0","0.0030257186081694403","0.0","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425457783131025408","https://twitter.com/LisaDeBruine/status/1425457783131025408","@dalejbarr @MaxMRabe I still can't figure out how to interpret the values from contr.poly. Is there a meaningful relationship between the min/max values such that you can interpret the estimate for the linear contrast, at least? https://t.co/IiS6OETSkZ","2021-08-11 14:03 +0000","366.0","5.0","0.01366120218579235","0.0","0.0","0.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","4","4","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425456669945995266","https://twitter.com/LisaDeBruine/status/1425456669945995266","@dalejbarr @MaxMRabe I've taken your advice, @dalejbarr, and it's called anova coding now. I've updated the functions and added examples of 2x2 and 2x3 designs with each of the coding types (except polynomial, which is 🤦‍♂️). 
https://t.co/zmqRzLjv2q","2021-08-11 13:58 +0000","368.0","6.0","0.016304347826086956","0.0","1.0","2.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425212714570375181","https://twitter.com/LisaDeBruine/status/1425212714570375181","@Mavumavu91 Two doses of Pfizer. I had a sore arm and felt a little tired for a day with the first one and nothing noticeable for the second.","2021-08-10 21:49 +0000","25.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425211897100521481","https://twitter.com/LisaDeBruine/status/1425211897100521481","@mdsumner I was panicking for a minute about what could possibly have changed so much about the process of creating packages to earn a whole new name!

Probably most users of usethis won't be too confused; it's obviously not an update. I don't get the need for the new NAs, though.","2021-08-10 21:46 +0000","617.0","4.0","0.006482982171799027","0.0","0.0","1.0","2.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425118779441393668","https://twitter.com/LisaDeBruine/status/1425118779441393668","@dalejbarr @MaxMRabe I' always thought from the part of your explainer that you screenshot above that deviation coding was just sum coding/2 (and that's how Alday explains it), but the examples in your post show the deviation coding for 3- and 5-level factors to equal treatment coding - 1/k.","2021-08-10 15:36 +0000","191.0","5.0","0.02617801047120419","0.0","0.0","0.0","0.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425118183061655555","https://twitter.com/LisaDeBruine/status/1425118183061655555","@dalejbarr @MaxMRabe I just mean the interpretation of the main effects isn't their difference from the grand mean (like in sum coding), but their difference from the baseline condition. (in this example, cat = 2, dog = 4, ferret = 9 and cat is the baseline). https://t.co/6QIJh4Eqxe","2021-08-10 15:33 +0000","339.0","11.0","0.032448377581120944","0.0","1.0","0.0","1.0","0.0","0.0","4.0","0.0","0","0","0","0","0","5","5","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425112051966427139","https://twitter.com/LisaDeBruine/status/1425112051966427139","@dalejbarr @MaxMRabe I’m confused now. The deviation coding you describe on talklab doesn’t have the same interpretation as sum coding (i.e., relative to the intercept), but has the same interpretation as treatment coding, except the intercept is the grand mean rather than the baseline.","2021-08-10 15:09 +0000","260.0","7.0","0.026923076923076925","0.0","1.0","0.0","0.0","0.0","0.0","5.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425110499138674698","https://twitter.com/LisaDeBruine/status/1425110499138674698","@dalejbarr @MaxMRabe I’m not sure I can conscience adding *another* term!  But I’m not sure there’s any other way out of this jingle-jangle mess.","2021-08-10 15:03 +0000","238.0","3.0","0.012605042016806723","0.0","1.0","0.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425096914849120262","https://twitter.com/LisaDeBruine/status/1425096914849120262","@hansijzerman @Flavio_Azevedo_ It's a short season and ends satisfyingly. You'll be able to tell if it's for you after the first episode.","2021-08-10 14:09 +0000","280.0","5.0","0.017857142857142856","0.0","0.0","2.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425096438074298370","https://twitter.com/LisaDeBruine/status/1425096438074298370","@simoncolumbus @cjvanlissa @davidrfeinberg @willemsleegers I loved LaTeX in the early 2000s, but I'm not a huge fan of PDFs anymore. HTML definitely wins for accessibility and interactivity. That's why I prefer Markdown.","2021-08-10 14:07 +0000","149.0","6.0","0.040268456375838924","0.0","1.0","1.0","0.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425096076713943041","https://twitter.com/LisaDeBruine/status/1425096076713943041","@MaxMRabe I don't necessarily agree with the terms under ""Other names"", I'm just listing what I've seen. Although I think that one might have been ""orthogonal polynomial"", so fair point.","2021-08-10 14:05 +0000","63.0","3.0","0.047619047619047616","0.0","0.0","1.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425092738417143812","https://twitter.com/LisaDeBruine/status/1425092738417143812","@_joaoverissimo @VerbingNouns @dalejbarr Thanks! Dale and Phillip define deviation coding differently  from each other and everyone else, convincing me that I can't sensibly use this term at all now.","2021-08-10 13:52 +0000","623.0","5.0","0.008025682182985553","0.0","0.0","2.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425090794550087680","https://twitter.com/LisaDeBruine/status/1425090794550087680","@cjvanlissa @simoncolumbus @davidrfeinberg A reproducible results section is a great hack for when your co-authors don’t use markdown or you need a group editing interface.

Wasn’t someone working on an interface like overleaf for RMarkdown?

There’s also @willemsleegers’ tidystats word plugin. https://t.co/NvmfL2cIIW","2021-08-10 13:44 +0000","114.0","4.0","0.03508771929824561","0.0","1.0","1.0","0.0","1.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425085091911544838","https://twitter.com/LisaDeBruine/status/1425085091911544838","@drboothroyd @HollyDunsworth The number of potential confounds in that thought experiment flooded my brain so fast I think it broke something.

(But also I’m sure my early ev psych work had equally badly justified assumptions.)","2021-08-10 13:22 +0000","141.0","7.0","0.04964539007092199","0.0","0.0","2.0","2.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425082871224705028","https://twitter.com/LisaDeBruine/status/1425082871224705028","@MaxMRabe I was originally going off @dalejbarr’s definition here https://t.co/7QFvm27iKc but I agree this doesn’t fit with most other uses of deviation coding. Simple is sometimes used for treatment coding, but this is just re-centred treatment coding. Do you differentiate sum &amp; effects?","2021-08-10 13:13 +0000","157.0","16.0","0.10191082802547771","0.0","1.0","0.0","1.0","1.0","0.0","13.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425074361732632576","https://twitter.com/LisaDeBruine/status/1425074361732632576","What should I call the deviation coding function? I'm settled on the rest of them, as they match the base R term. But every term I've found for deviation coding is also used for another type!
https://t.co/zmqRzLjv2q https://t.co/z77mRKo2pq","2021-08-10 12:39 +0000","4408.0","252.0","0.05716878402903811","1.0","6.0","11.0","6.0","24.0","0.0","81.0","0.0","0","0","0","0","0","123","123","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425019731782914055","https://twitter.com/LisaDeBruine/status/1425019731782914055","@drboothroyd I totally agree. That’s what I’m trying to make easier with the naming scheme in the new faux functions:

https://t.co/q5npUTbVQy","2021-08-10 09:02 +0000","130.0","2.0","0.015384615384615385","0.0","0.0","0.0","0.0","2.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1425019330765508633","https://twitter.com/LisaDeBruine/status/1425019330765508633","@hansijzerman I think we might have very different tastes :) Have you seen Russian Doll?","2021-08-10 09:00 +0000","254.0","7.0","0.027559055118110236","0.0","1.0","1.0","1.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424859077214101508","https://twitter.com/LisaDeBruine/status/1424859077214101508","@emilynordmann @KathleenCronie @pimpmymemory What do I choose if I always mean to turn on music, get distracted, and realise 3 hours later that Spotify is still sitting paused on my TV screen? (This is why there’s a play button burned into my TV.)","2021-08-09 22:24 +0000","733.0","19.0","0.02592087312414734","0.0","2.0","5.0","2.0","0.0","0.0","10.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424856276463464455","https://twitter.com/LisaDeBruine/status/1424856276463464455","@jamesheathers Weirdly, I can lower that arm to almost parallel with 3x the weight (which is still lame, I am fully aware). But I don’t think my lateral raises ever got above 9kg before.","2021-08-09 22:13 +0000","281.0","4.0","0.014234875444839857","0.0","0.0","0.0","0.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424853800561647616","https://twitter.com/LisaDeBruine/status/1424853800561647616","@bmwiernik @sharoz This useful article by @force11rescomm recommends:

“If an article exists that describes the software, it should be cited as an additional reference, as well as citing the software itself. Do not cite the article instead of the software.”

https://t.co/N5zX8WyMTU","2021-08-09 22:03 +0000","1031.0","29.0","0.028128031037827354","3.0","0.0","5.0","1.0","10.0","0.0","10.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424851660032118786","https://twitter.com/LisaDeBruine/status/1424851660032118786","@jamesheathers I’m also concerned, but the advice from the NHS physio (who I’ve only had phone convos with) is to do whatever I’m comfortable with. Perhaps it’s time to pay to see a proper sports physio. Anyways, be careful with beat saber so you don’t end up like me!","2021-08-09 21:54 +0000","389.0","6.0","0.015424164524421594","0.0","1.0","0.0","2.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424843171775852544","https://twitter.com/LisaDeBruine/status/1424843171775852544","I want to start lifting after a 1/2-year shoulder injury. Does anyone have advice on asymmetric strength? For example, my dumbbell lateral raise is 4kg on the left, but I’m struggling with 0.5kg on the right. My dumbbell bench is 7/4. Should I avoid bar lifts until I’m more even?","2021-08-09 21:20 +0000","3947.0","184.0","0.04661768431720294","0.0","6.0","0.0","12.0","0.0","0.0","166.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424825202383499267","https://twitter.com/LisaDeBruine/status/1424825202383499267","@hansijzerman What kind of shows do you tend to like?","2021-08-09 20:09 +0000","656.0","7.0","0.010670731707317074","0.0","1.0","1.0","2.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424824777127120899","https://twitter.com/LisaDeBruine/status/1424824777127120899","@ukrepro @DrGBuckingham @CumberlandLodge So sorry I couldn’t join you!","2021-08-09 20:07 +0000","265.0","7.0","0.026415094339622643","0.0","1.0","1.0","4.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424823192707813377","https://twitter.com/LisaDeBruine/status/1424823192707813377","@DrGBuckingham Britons clearly have more experience with geese.","2021-08-09 20:01 +0000","749.0","9.0","0.012016021361815754","0.0","1.0","6.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424822931364974594","https://twitter.com/LisaDeBruine/status/1424822931364974594","@somacrat @wdonald_1985 I totally get this, but you’ve developed skills that I really hope (and optimistically expect) are going to be increasingly rewarded in the future. And a future meta-analyst is going to love you.","2021-08-09 20:00 +0000","1705.0","52.0","0.030498533724340176","0.0","1.0","27.0","11.0","0.0","0.0","13.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424775071369998338","https://twitter.com/LisaDeBruine/status/1424775071369998338","@Littlejohnsband I can’t figure out from the various sources I’ve been recommended what effects coding actually is. Barlez uses it as the category that includes sum and simple/contrast coding (which I’m calling deviation but would normally call effects coding). What is it for you?","2021-08-09 16:50 +0000","82.0","2.0","0.024390243902439025","0.0","1.0","0.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424774434095779851","https://twitter.com/LisaDeBruine/status/1424774434095779851","@Littlejohnsband I assume you mean orthogonal == polynomial? Although Davis divides Contrast coding into orthogonal and non-orthogonal. I haven’t quite figured out how his taxonomy maps onto everyone else’s. https://t.co/6LgyXAQU3X","2021-08-09 16:47 +0000","51.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424760853979701253","https://twitter.com/LisaDeBruine/status/1424760853979701253","Thanks everyone who helped my confusion (both increasing and decreasing it 🙃). Here’s where I am now with my own set of functions to set contrasts and a vignette to explain how to interpret them. 

https://t.co/q5npUTbVQy https://t.co/nB9f2bwMzj https://t.co/HsmWsQ4wqF","2021-08-09 15:53 +0000","6326.0","265.0","0.04189061018020866","5.0","3.0","35.0","5.0","45.0","0.0","49.0","0.0","0","0","0","0","0","123","123","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424755230676238349","https://twitter.com/LisaDeBruine/status/1424755230676238349","@Limor_Raviv @dalejbarr Oh no! Their simple coding is @dalejbarr 's deviation coding and Barlaz's contrast coding, and their deviation coding is everyone else's sum coding.

Also, why does everyone describe this coding as ""-1/k and (k-1)/k"" instead of the much easier ""treatment coding minus 1/k""?","2021-08-09 15:31 +0000","241.0","10.0","0.04149377593360996","0.0","1.0","3.0","1.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424722144613785602","https://twitter.com/LisaDeBruine/status/1424722144613785602","@FredOswald @dalejbarr Nice! Although this now conflicts with Barlaz's blog, which lists contrast coding and sum coding as instances of effects coding :) https://t.co/L2Eb885xSg","2021-08-09 13:20 +0000","181.0","19.0","0.10497237569060773","1.0","1.0","0.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","16","16","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424719328801443847","https://twitter.com/LisaDeBruine/status/1424719328801443847","@FredOswald @dalejbarr I cite her great article at the top of my vignette as the inspiration for these functions. But also the source of my confusion about contrast/effect coding terminology :)

https://t.co/q5npUTbVQy","2021-08-09 13:08 +0000","188.0","12.0","0.06382978723404255","0.0","1.0","3.0","1.0","6.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424683791738675200","https://twitter.com/LisaDeBruine/status/1424683791738675200","@tjmahr Oh, those are really helpful! Especially in my quest to figure out what the deal is with contr.poly (is the scaling for each contrast meaningful? why does emmeans:::poly.emmc use integers?)","2021-08-09 10:47 +0000","154.0","4.0","0.025974025974025976","0.0","1.0","1.0","0.0","1.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424681679826915330","https://twitter.com/LisaDeBruine/status/1424681679826915330","What would you call this type of coding? (contr.helmert) https://t.co/0SqDhCToM1","2021-08-09 10:39 +0000","1956.0","51.0","0.02607361963190184","0.0","4.0","1.0","0.0","0.0","0.0","22.0","0.0","0","0","0","0","0","24","24","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424681674957262849","https://twitter.com/LisaDeBruine/status/1424681674957262849","What would you call this type of coding? (contr.treatment) https://t.co/fmlEq6XzRF","2021-08-09 10:39 +0000","2141.0","71.0","0.033162073797290983","0.0","7.0","1.0","1.0","0.0","0.0","17.0","0.0","0","0","0","0","0","45","45","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424681670213578756","https://twitter.com/LisaDeBruine/status/1424681670213578756","What would you call this type of coding? (contr.sum) https://t.co/ZyZAT2GNCE","2021-08-09 10:39 +0000","765.0","26.0","0.03398692810457516","0.0","4.0","1.0","1.0","0.0","0.0","8.0","0.0","0","0","0","0","0","12","12","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424681664626704392","https://twitter.com/LisaDeBruine/status/1424681664626704392","What would you call this type of coding? (effect coding? contrast coding? simple contrasts? It's contr.treatment minus 1/n so the intercept is the grand mean) https://t.co/fj0gbPiNKH","2021-08-09 10:39 +0000","838.0","53.0","0.06324582338902147","0.0","4.0","1.0","0.0","0.0","0.0","18.0","0.0","0","0","0","0","0","30","30","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424681662030430217","https://twitter.com/LisaDeBruine/status/1424681662030430217","What would you call this type of coding? (MASS::contr.sdif divided by n for better interpretability) https://t.co/fhm5dTtEBX","2021-08-09 10:39 +0000","675.0","59.0","0.0874074074074074","0.0","1.0","1.0","0.0","0.0","0.0","12.0","0.0","0","0","0","0","0","45","45","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424681657173430272","https://twitter.com/LisaDeBruine/status/1424681657173430272","I'm starting a thread to figure out what people call different contrasts. I can't find a definitive guide and many of the resources I'm finding on the web use different names. @dalejbarr I blame you for leading me to believe that effect coding is a thing other people  know :)","2021-08-09 10:39 +0000","12533.0","268.0","0.0213835474347722","2.0","6.0","23.0","12.0","6.0","0.0","203.0","0.0","0","0","0","0","0","16","16","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424651012170735617","https://twitter.com/LisaDeBruine/status/1424651012170735617","@AchimZeileis Thanks! I just checked it out and contr.sdif() is really close to what I implemented as contrast_code(), just with successive levels as the reference rather than a fixed reference. I can see how that would be useful.","2021-08-09 08:37 +0000","62.0","2.0","0.03225806451612903","0.0","0.0","1.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424647200605446154","https://twitter.com/LisaDeBruine/status/1424647200605446154","Also, are there any other common contrasts people use that aren’t covered by contrast/effect, treatment/dummy, helmert, and sum coding?","2021-08-09 08:22 +0000","1655.0","12.0","0.0072507552870090634","0.0","4.0","0.0","2.0","0.0","0.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424645824697536514","https://twitter.com/LisaDeBruine/status/1424645824697536514","@siminevazire 🤣 Good luck! I recently did C25K for the third time in 12 years. I love how it eases you back to running in a way that doesn’t immediately result in knee injury, which is what always happens when I try to restart on my own.","2021-08-09 08:16 +0000","819.0","6.0","0.007326007326007326","0.0","0.0","1.0","5.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424638249679654914","https://twitter.com/LisaDeBruine/status/1424638249679654914","@aussieweegie Hard same. Work doesn’t even start for 15 minutes and I’m considering getting on the HR page to book more :)","2021-08-09 07:46 +0000","395.0","4.0","0.010126582278481013","0.0","0.0","1.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424515217413787648","https://twitter.com/LisaDeBruine/status/1424515217413787648","I posted a vignette for the functions I'm working on. I think their names make interpreting a linear model much easier.
https://t.co/q5npUTbVQy","2021-08-08 23:37 +0000","1177.0","12.0","0.010195412064570943","0.0","0.0","1.0","1.0","9.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424514806619516931","https://twitter.com/LisaDeBruine/status/1424514806619516931","The functions were inspired by Marissa Barlaz's great post (https://t.co/DPhod45KyL), but I'm wondering what terms everyone  uses. Her contrast coding, I'd normally call effect coding. And her Reverse Helmert is closer to contrast.helmert() than her Helmert (but not identical).","2021-08-08 23:36 +0000","2075.0","9.0","0.004337349397590362","0.0","1.0","2.0","0.0","5.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424514804719538179","https://twitter.com/LisaDeBruine/status/1424514804719538179","Thanks everyone who gave advice. I ended up making my own functions. Feedback on the vignette would be very welcome. I feel like this clarified coding schemes for me a lot. https://t.co/q5npUTbVQy https://t.co/Kh3MB0xwlY","2021-08-08 23:36 +0000","4130.0","68.0","0.01646489104116223","1.0","1.0","10.0","6.0","25.0","0.0","25.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424481652894535684","https://twitter.com/LisaDeBruine/status/1424481652894535684","@HenrikSingmann @nicholaraihani Thanks! The 3 functions in car have much better names than the stats versions (which I was using). But I also want to provide contrast coding (sensu https://t.co/DPhod45KyL), and don't need another dependency that isn't exactly what I want, so I'll continue with mine.","2021-08-08 21:24 +0000","105.0","5.0","0.047619047619047616","0.0","0.0","2.0","1.0","2.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424457927289675781","https://twitter.com/LisaDeBruine/status/1424457927289675781","@nicholaraihani @HenrikSingmann It's got functins like set_effects_contrasts() to globally set the contrasts for an anova, but I want to be able to individually set the contrasts for factors in lmer, so that, e.g., one factor could be effect-coded and another treatment-coded.","2021-08-08 19:50 +0000","379.0","2.0","0.005277044854881266","0.0","1.0","0.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424457287700209668","https://twitter.com/LisaDeBruine/status/1424457287700209668","@minigamedev This is my favourite explainer https://t.co/DPhod45KyL","2021-08-08 19:47 +0000","155.0","44.0","0.2838709677419355","0.0","0.0","6.0","0.0","37.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424455342692487176","https://twitter.com/LisaDeBruine/status/1424455342692487176","Is there already an #rstats function that will apply contrast, sum, or treatment coding to a factor with automatically calculated, meaningful contrast names? I'm working on some for faux like the ones below, but I don't want to reinvent the wheel. https://t.co/zaxhwCdAXw","2021-08-08 19:39 +0000","10891.0","335.0","0.030759342576439263","6.0","5.0","13.0","8.0","4.0","6.0","136.0","0.0","0","0","0","0","0","157","157","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424357321401159683","https://twitter.com/LisaDeBruine/status/1424357321401159683","@ChelseaMDO1 If you like savoury, try smoked paprika and nutritional yeast. (I'm totally going to go bake a potato now and have it with that and greek yoghurt.)","2021-08-08 13:10 +0000","418.0","4.0","0.009569377990430622","0.0","0.0","1.0","2.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424334178141753348","https://twitter.com/LisaDeBruine/status/1424334178141753348","I quite like the S3 system in #rstats, but I also totally agree with this:

“S3 is very flexible, which means it allows you to do things that are quite ill-advised.” 😂","2021-08-08 11:38 +0000","2263.0","40.0","0.017675651789659744","1.0","2.0","3.0","2.0","0.0","1.0","22.0","0.0","0","0","0","0","0","9","9","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424329299335827458","https://twitter.com/LisaDeBruine/status/1424329299335827458","@thuyvytnguyen @drboothroyd It is a risky assumption, but also unfair to students who were taught well and learned what they were meant to, if they have to waste a term reviewing undergrad stats. I’d love to have different classes based on experience, but there are barriers to implementing that in an MSc.","2021-08-08 11:19 +0000","287.0","7.0","0.024390243902439025","0.0","2.0","4.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424328430385041409","https://twitter.com/LisaDeBruine/status/1424328430385041409","@nic_crane Argument order and defaults are important, too. Changing those can mess up someone else’s code and introduce irreproducibility. (I know we should all use renv or containers if we care about reproducibility, but I also get the barriers to that.)","2021-08-08 11:15 +0000","125.0","1.0","0.008","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424324030593703936","https://twitter.com/LisaDeBruine/status/1424324030593703936","I think I’ll be using purrr::quietly() a lot more in my data simulation to check for lmer warnings. 

“quietly(): turns output, messages, and warning side-effects into output, message, and warning components of the output.”","2021-08-08 10:58 +0000","1934.0","12.0","0.0062047569803516025","0.0","1.0","5.0","0.0","0.0","0.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424323172296495107","https://twitter.com/LisaDeBruine/status/1424323172296495107","One of the main things I’m learning from this book is that I should learn all of the purrr functions. I’d been making my own version of transpose() for years to flip nested lists:

x = list(
 list(a=1, b=T),
 list(a=2, b=F)
)

transpose(x)
#&gt; list(a = list(1, 2), b = list(T, F))","2021-08-08 10:54 +0000","1133.0","8.0","0.00706090026478376","0.0","1.0","1.0","3.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424318283981987843","https://twitter.com/LisaDeBruine/status/1424318283981987843","@nic_crane I’d also recommend having pretty thorough test coverage before submitting.

Beware that CRAN requires check.environment=FALSE if you use all.equal() or testthat::expect_equal() to compare functions, or your tests will fail on devel. 

https://t.co/DM6fZ8Xr4g","2021-08-08 10:35 +0000","575.0","16.0","0.02782608695652174","0.0","1.0","2.0","1.0","7.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424317220142866437","https://twitter.com/LisaDeBruine/status/1424317220142866437","@nic_crane I regret not really thinking through the naming of functions and arguments before I submitted faux to CRAN.","2021-08-08 10:31 +0000","533.0","9.0","0.016885553470919325","0.0","2.0","1.0","1.0","0.0","0.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424291085434736645","https://twitter.com/LisaDeBruine/status/1424291085434736645","@fiona_jordan That’s gorgeous. It looks like a mix of crochet and tatting.","2021-08-08 08:47 +0000","399.0","2.0","0.005012531328320802","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1424288212579766274","https://twitter.com/LisaDeBruine/status/1424288212579766274","@MadonaMac @WindheartCogs The scores are relative to the original data, which probably didn’t include a bunch of people on Twitter who self-selected into being interested in this. You can access the paper and data at the original link. https://t.co/3xIBhVkMQZ","2021-08-08 08:35 +0000","64.0","4.0","0.0625","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","3","3","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423987525937508357","https://twitter.com/LisaDeBruine/status/1423987525937508357","Now for the meta-commentary. *Why* is this so fun? It has literally hundreds of quote-tweets proclaiming it fun. The construct validity for measuring creativity seems dodgy, and the norming data are clearly not from a population like my Twitter network. But it’s demonstrably fun! https://t.co/NSnEICJcP7","2021-08-07 12:40 +0000","5993.0","83.0","0.013849491072918404","1.0","2.0","15.0","10.0","0.0","0.0","55.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423975792959180800","https://twitter.com/LisaDeBruine/status/1423975792959180800","@rasmansa Hence the memetic “object of type closure not subsettable” error when you try to subset a data frame called data, but forgot to make it, so you’re trying to subset the data() function. https://t.co/uOM2jv4ulB","2021-08-07 11:54 +0000","79.0","5.0","0.06329113924050633","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","4","4","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423974826813894659","https://twitter.com/LisaDeBruine/status/1423974826813894659","@rasmansa All functions in R are closures, so that wouldn’t help differentiate typical functions from function factories. The book does discuss closures in an earlier chapter.","2021-08-07 11:50 +0000","75.0","5.0","0.06666666666666667","0.0","1.0","1.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423949991828197378","https://twitter.com/LisaDeBruine/status/1423949991828197378","I love that you can use this to make a counter (although I’m sure there are better patterns for anything I’d use it for):

new_counter &lt;- function() {
  i &lt;- 0
  function() {
    i &lt;&lt;- i + 1
    i
  }
}

c1 &lt;- new_counter()
c1() # 0
c1() # 1","2021-08-07 10:11 +0000","1778.0","13.0","0.007311586051743532","0.0","2.0","2.0","1.0","0.0","0.0","8.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423944562108731393","https://twitter.com/LisaDeBruine/status/1423944562108731393","@jeroendstout It would probably be pretty difficult to filter out proper noun usage from the corpus. Maybe remove words with initial caps unless they’re at the start of a sentence?","2021-08-07 09:50 +0000","26.0","2.0","0.07692307692307693","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423942756351844352","https://twitter.com/LisaDeBruine/status/1423942756351844352","@msmelek https://t.co/xQv0yHM8xw","2021-08-07 09:43 +0000","159.0","2.0","0.012578616352201259","0.0","0.0","1.0","0.0","1.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423942549924896769","https://twitter.com/LisaDeBruine/status/1423942549924896769","@WindheartCogs The original paper (whose distribution we’re being scored against) tested a more diverse population than my extended Twitter followers, I’m guessing. https://t.co/cwiz2HSrwI","2021-08-07 09:42 +0000","1568.0","185.0","0.11798469387755102","0.0","1.0","15.0","5.0","0.0","0.0","3.0","0.0","0","0","0","0","0","161","161","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423941421501927431","https://twitter.com/LisaDeBruine/status/1423941421501927431","@pyromuffin Static and script have alternate meanings related to coding, so maybe that’s how they ended up associated?","2021-08-07 09:37 +0000","499.0","6.0","0.012024048096192385","0.0","1.0","0.0","1.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423939382151720961","https://twitter.com/LisaDeBruine/status/1423939382151720961","@jeroendstout Scion and Hulk are both Marvel characters (but I don’t think they’ve ever been in the same comic). So I guess it depends where their corpus comes from.","2021-08-07 09:29 +0000","270.0","3.0","0.011111111111111112","0.0","1.0","1.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423934522580684801","https://twitter.com/LisaDeBruine/status/1423934522580684801","@octonion Can you make a plot of the distribution of scores for randomly chosen lists of valid words?","2021-08-07 09:10 +0000","417.0","4.0","0.009592326139088728","0.0","1.0","1.0","0.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423934216778260483","https://twitter.com/LisaDeBruine/status/1423934216778260483","@traan_sient Ah, I see what you meant. It would also be nice to see the distribution of scores for words randomly chosen from the list of valid words to see where you are relative to the theoretical max.","2021-08-07 09:09 +0000","39.0","2.0","0.05128205128205128","0.0","0.0","1.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423932808867758083","https://twitter.com/LisaDeBruine/status/1423932808867758083","@hisotalus It’s got an associated paper with open data and code if you want to check. I think there’s a pretty huge selection effect (my followers are mainly academics) and my original post probably anchored the threshold for posting scores. 

https://t.co/C8CezOEa9m https://t.co/IPQkWPMB1g","2021-08-07 09:03 +0000","417.0","37.0","0.08872901678657075","0.0","1.0","5.0","0.0","5.0","0.0","5.0","0.0","0","0","0","0","0","21","21","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423931329662570497","https://twitter.com/LisaDeBruine/status/1423931329662570497","@AdamRutherford The 4th wall is normally only permeable for Fleabag, so I think it counts when the hot priest breaks it. https://t.co/6w6SztZ9y6","2021-08-07 08:57 +0000","528.0","48.0","0.09090909090909091","0.0","1.0","7.0","3.0","5.0","0.0","8.0","0.0","0","0","0","0","0","24","24","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423927378510757890","https://twitter.com/LisaDeBruine/status/1423927378510757890","@traan_sient There’s only 720 unique 7-word combos out of the 10 words (or fewer if one or more of your words were invalid), so I’m just suggesting calculating them all and showing the best one.","2021-08-07 08:41 +0000","116.0","3.0","0.02586206896551724","0.0","1.0","2.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423679922753658884","https://twitter.com/LisaDeBruine/status/1423679922753658884","Second try. I wish it would try all of the valid 7-word combos and tell you the best (or distribution). https://t.co/i2yZwg9d1O","2021-08-06 16:18 +0000","43457.0","3288.0","0.07566099822813356","0.0","7.0","110.0","57.0","35.0","0.0","141.0","0.0","0","0","0","0","0","2938","2938","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423645341900058626","https://twitter.com/LisaDeBruine/status/1423645341900058626","Would you watch me code a shiny app?","2021-08-06 14:01 +0000","1401.0","66.0","0.047109207708779445","1.0","0.0","1.0","8.0","0.0","0.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423645339479838720","https://twitter.com/LisaDeBruine/status/1423645339479838720","Would anyone be interested if I did Twitch livestreams of making a #rstats #shiny app? I want to work on an app for simulating data using faux. https://t.co/jbRGMNMZwg","2021-08-06 14:01 +0000","12462.0","170.0","0.01364147006900979","4.0","5.0","55.0","28.0","0.0","1.0","77.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423644004240039939","https://twitter.com/LisaDeBruine/status/1423644004240039939","This is useful. I got myself off some lists about crypto, which seems to be a theme among the “hi guys” DMs I get. Most look legit, and I’m amused by the one I’m on called “Open Science Talking Heads” :) https://t.co/N1cigVnRmw","2021-08-06 13:55 +0000","3527.0","16.0","0.004536433229373405","0.0","0.0","6.0","3.0","0.0","0.0","7.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423639120694456322","https://twitter.com/LisaDeBruine/status/1423639120694456322","I actually said “ooh, ooh!” out loud when I read this at the start of the Function Factories chapter. I have so many ideas for what to do with it! https://t.co/OIuGdFVgKL","2021-08-06 13:36 +0000","2799.0","68.0","0.024294390853876385","0.0","2.0","4.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","61","61","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423628646888136708","https://twitter.com/LisaDeBruine/status/1423628646888136708","I’m so excited to see this short animated film on #prosopagnosia by the very talented @stevenfraserart, whose other work on identity and neurodivergence is also well worth checking out.

(Tagging @faceblinduk @ProfSarahBate @twf_team) https://t.co/6vsNofT4xi","2021-08-06 12:54 +0000","2554.0","17.0","0.006656225528582615","1.0","1.0","4.0","4.0","0.0","2.0","5.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423616261745950724","https://twitter.com/LisaDeBruine/status/1423616261745950724","@smallcygnet @AndyPerfors What’s your most disgusting 3-food combo?","2021-08-06 12:05 +0000","50.0","5.0","0.1","0.0","1.0","1.0","1.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423581871083069441","https://twitter.com/LisaDeBruine/status/1423581871083069441","@FrederikAust @ceptional https://t.co/Pp6GLTx7wv","2021-08-06 09:49 +0000","274.0","33.0","0.12043795620437957","0.0","2.0","1.0","2.0","22.0","0.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423465702791462912","https://twitter.com/LisaDeBruine/status/1423465702791462912","@GeorgieSDOH I think the percentiles are against the published data. Otherwise, everyone posting in my mentions is going to ruin their curve, since they’re almost all scoring in the high 90 percentiles!","2021-08-06 02:07 +0000","4596.0","38.0","0.008268059181897302","1.0","2.0","8.0","12.0","0.0","0.0","15.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423452344231079943","https://twitter.com/LisaDeBruine/status/1423452344231079943","https://t.co/1Q8O0Jo4mq","2021-08-06 01:14 +0000","71270.0","8520.0","0.11954539076750385","2.0","9.0","67.0","105.0","68.0","0.0","314.0","0.0","0","0","0","0","0","7955","7955","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423445172092866563","https://twitter.com/LisaDeBruine/status/1423445172092866563","Oh, this was fun! You think of the ten least-related nouns possible. I scored in the 94th percentile. 

https://t.co/FhR4DR38OU","2021-08-06 00:45 +0000","1020329.0","77718.0","0.07616954923362955","1093.0","194.0","1190.0","826.0","33374.0","0.0","36697.0","0.0","0","0","0","0","0","4344","4344","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423388941969920006","https://twitter.com/LisaDeBruine/status/1423388941969920006","@DictionaryAge @jfrickuga Who are these peers? Where are their reviews being documented? PubPeer?

What do you think the construct is that is being measured by parental observations (which are impressions unless they are literal mind readers)?","2021-08-05 21:02 +0000","40.0","9.0","0.225","0.0","1.0","0.0","2.0","0.0","0.0","6.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423388490633564165","https://twitter.com/LisaDeBruine/status/1423388490633564165","@jd_wilko Was it @annloh’s paper? https://t.co/t3LRlGuQSX","2021-08-05 21:00 +0000","1566.0","32.0","0.020434227330779056","1.0","1.0","3.0","0.0","18.0","0.0","9.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423387585460776961","https://twitter.com/LisaDeBruine/status/1423387585460776961","@DictionaryAge @jfrickuga The paper suffers from terminal measurement invalidity, so its data can never be used to support the type of conclusions it makes. You can only conclude things about parental impressions, not about children’s actual experiences.","2021-08-05 20:57 +0000","139.0","6.0","0.04316546762589928","1.0","1.0","0.0","2.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423360062773506049","https://twitter.com/LisaDeBruine/status/1423360062773506049","@DictionaryAge @jfrickuga Can you quantify what you mean by “peer reviewed more than any other in recent history”?","2021-08-05 19:07 +0000","58.0","3.0","0.05172413793103448","0.0","1.0","0.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423265716518666242","https://twitter.com/LisaDeBruine/status/1423265716518666242","@Faentomet @m0mimpregnant @jensayingthings That’s the origin of my milk art. I started just writing my name in Sharpie, then it escalated. It never really stopped people using my milk, but was fun. https://t.co/x44OEOFC2F","2021-08-05 12:52 +0000","472.0","107.0","0.2266949152542373","0.0","0.0","10.0","0.0","0.0","0.0","1.0","0.0","0","0","0","0","0","96","96","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423263941841637380","https://twitter.com/LisaDeBruine/status/1423263941841637380","@emilynordmann Whether or not people are supertasters? There’s a bunch of dodgy-looking personality and food preference work, so depends what you’re trying to demonstrate, but it’s a quirky one that is unlikely to be a sensitive topic. 

https://t.co/d712I42ujD","2021-08-05 12:45 +0000","238.0","3.0","0.012605042016806723","0.0","0.0","2.0","0.0","1.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423261973844828169","https://twitter.com/LisaDeBruine/status/1423261973844828169","@scotchfairy @hogotheforsaken @catvalente @Thereisalwaysh I thought “Karen” was originally specifically about white women weaponising their femininity against POC, and the change to any complaining white woman was mainly caused by white women themselves starting to use the term?

https://t.co/LLyBXklYIH","2021-08-05 12:37 +0000","120.0","9.0","0.075","0.0","3.0","1.0","0.0","2.0","0.0","2.0","0.0","0","0","0","0","0","1","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423258277413011457","https://twitter.com/LisaDeBruine/status/1423258277413011457","@thuyvytnguyen @drboothroyd @MichelleAKline That’s not an adjective that should apply to your office!!! 😬","2021-08-05 12:23 +0000","223.0","7.0","0.03139013452914798","0.0","1.0","2.0","1.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423243119529795586","https://twitter.com/LisaDeBruine/status/1423243119529795586","@drboothroyd @MichelleAKline Before FEC, did UKRI grants require researcher salary? I just applied for a multi-national grant where each PI had to follow the rules of their own funder. I had to apply for my salary, while my Canadian colleague wasn’t allowed to.","2021-08-05 11:22 +0000","259.0","8.0","0.03088803088803089","0.0","2.0","1.0","1.0","0.0","0.0","4.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423236651296362501","https://twitter.com/LisaDeBruine/status/1423236651296362501","@MichelleAKline Yup. And my uni prohibits me from applying for UKRI grants unless a minimum percentage of my time is in the budget and I have a minimum percentage of postdoc time because that has overheads. Now that I’m a prof, sometimes my time is too expensive to be able to apply for a grant.","2021-08-05 10:57 +0000","1365.0","47.0","0.034432234432234435","1.0","3.0","5.0","6.0","0.0","0.0","32.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423234187709108224","https://twitter.com/LisaDeBruine/status/1423234187709108224","@KhandisBlake @SpeciesTypical I was confused when first called “buddy” by a Glaswegian cab driver, but I love it now. Also “pal” is a gender-neutral and non-sarcastic form of address here.","2021-08-05 10:47 +0000","177.0","10.0","0.05649717514124294","0.0","1.0","5.0","2.0","0.0","0.0","2.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423229945115983872","https://twitter.com/LisaDeBruine/status/1423229945115983872","@AnnaHenschel The app is here and saves data to a Googlesheet. I’m working on a book that covers the details and will send that and the link to the source code by DM. 

https://t.co/amTeyNUBRh","2021-08-05 10:30 +0000","206.0","11.0","0.05339805825242718","0.0","1.0","1.0","1.0","5.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423026117821308936","https://twitter.com/LisaDeBruine/status/1423026117821308936","OMG. I have so much code with for loops over i in seq_along(v) so I can reference both the names and values in a vector. And imap was there the whole time! https://t.co/oFScbTCoTn","2021-08-04 21:00 +0000","4290.0","291.0","0.06783216783216783","3.0","2.0","23.0","5.0","1.0","0.0","15.0","0.0","0","0","0","0","0","242","242","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423025449064620033","https://twitter.com/LisaDeBruine/status/1423025449064620033","@VerbingNouns I think I checked my book right after.","2021-08-04 20:58 +0000","52.0","3.0","0.057692307692307696","0.0","0.0","1.0","2.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423021995483144193","https://twitter.com/LisaDeBruine/status/1423021995483144193","purrr::pluck() does what I’ve tried to code myself several times to make selecting missing elements work consistently and with defaults. I’m glad to know it exists and I can stop making inferior versions. https://t.co/Koaj7gAR1f","2021-08-04 20:44 +0000","3096.0","115.0","0.03714470284237726","2.0","2.0","7.0","2.0","2.0","0.0","5.0","0.0","0","0","0","0","0","95","95","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423018118327291907","https://twitter.com/LisaDeBruine/status/1423018118327291907","@VerbingNouns Good luck! My favourite question was “Who is the head of the Church of England?” (The Queen, but who cares?) Although I was actually pregnant when I took it, I still got the one wrong about all the places you can get antenatal care from.","2021-08-04 20:28 +0000","46.0","2.0","0.043478260869565216","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423017162181160962","https://twitter.com/LisaDeBruine/status/1423017162181160962","@ajamesgreen Yup. Seems to be a mathematician thing. https://t.co/uPx4R258dM","2021-08-04 20:25 +0000","778.0","80.0","0.10282776349614396","1.0","2.0","7.0","3.0","67.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423016686131851265","https://twitter.com/LisaDeBruine/status/1423016686131851265","This is going to be useful for a faux function where I’d like to be able to override values in a design object if people explicitly set an argument, but not if the argument was set from defaults. I thought I’d have to change all defaults to NULL and set them in the function. https://t.co/6T3WcnrFtT","2021-08-04 20:23 +0000","2618.0","80.0","0.030557677616501147","2.0","1.0","10.0","3.0","1.0","0.0","7.0","0.0","0","0","0","0","0","56","56","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423015595252109312","https://twitter.com/LisaDeBruine/status/1423015595252109312","@joieprout It definitely is. I tried googling for twiddle and rstats, but I think this might be a really esoteric name for the tilde. 

https://t.co/ZZqUZ69KW4","2021-08-04 20:18 +0000","688.0","52.0","0.0755813953488372","0.0","0.0","10.0","2.0","31.0","0.0","9.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423015073208020995","https://twitter.com/LisaDeBruine/status/1423015073208020995","You can set default values in arguments to the value of other arguments or even values you’ll calculate later in the function, as long as you don’t reference that argument until the value is created. https://t.co/XbC4D3OcRV","2021-08-04 20:16 +0000","6629.0","304.0","0.04585910393724544","4.0","2.0","13.0","1.0","0.0","0.0","33.0","0.0","0","0","0","0","0","251","251","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423013888082251778","https://twitter.com/LisaDeBruine/status/1423013888082251778","The full source reference of functions, including comments, is in the srcref attribute. https://t.co/wd9AaKBmh2","2021-08-04 20:12 +0000","2041.0","207.0","0.10142087212150906","1.0","1.0","4.0","4.0","4.0","0.0","4.0","0.0","0","0","0","0","0","189","189","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1423013115747311617","https://twitter.com/LisaDeBruine/status/1423013115747311617","I’m reading Advanced R and am going to make a thread of interesting things I learned. The first one is that Hadley Wickham thinks ~ is pronounced “twiddle” 🤣 https://t.co/yemSeEcNmQ","2021-08-04 20:08 +0000","27237.0","2505.0","0.09197048133054302","10.0","6.0","97.0","42.0","17.0","0.0","974.0","0.0","0","0","0","0","0","1359","1359","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422980551825399813","https://twitter.com/LisaDeBruine/status/1422980551825399813","@zortea_tiago @UniofOxford Best of luck in your journey and I hope we see you again in Glasgow periodically. https://t.co/HFRkFkPPQX","2021-08-04 17:59 +0000","489.0","2.0","0.00408997955010225","0.0","1.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","57","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422979299045425163","https://twitter.com/LisaDeBruine/status/1422979299045425163","@JkayFlake @joshtybur Lieberman’s moral wrongness “scale” for assessing sibling incest avoidance (via the mean ranking of the two highlighted items) is certainly an *interesting* one, and definitely lends itself to critique. 

https://t.co/lattM2dnR8 https://t.co/tlHlklQXIJ","2021-08-04 17:54 +0000","488.0","25.0","0.05122950819672131","0.0","0.0","1.0","2.0","2.0","0.0","3.0","0.0","0","0","0","0","0","17","17","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422976956803473411","https://twitter.com/LisaDeBruine/status/1422976956803473411","@JkayFlake The EMBU or its short form for assessing the parent-child relationship. https://t.co/f0MZKNUeUr","2021-08-04 17:45 +0000","839.0","12.0","0.014302741358760428","0.0","0.0","1.0","0.0","11.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422976204928344065","https://twitter.com/LisaDeBruine/status/1422976204928344065","@JkayFlake Three domains of disgust scale by @joshtybur et al. https://t.co/DSTSPapL4J (and several associated validity studies)","2021-08-04 17:42 +0000","851.0","22.0","0.025851938895417155","0.0","2.0","5.0","3.0","5.0","0.0","7.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422975452793163776","https://twitter.com/LisaDeBruine/status/1422975452793163776","@JkayFlake @BorsboomDenny @matherion @aesthetix777 You should do a zoom reading group while you’re searching for good papers. That’s a topic I wish I’d learned about, but missed with a non-traditional pathway into psych.","2021-08-04 17:39 +0000","443.0","8.0","0.01805869074492099","0.0","1.0","4.0","0.0","0.0","0.0","3.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422675134012432389","https://twitter.com/LisaDeBruine/status/1422675134012432389","Advanced R wouldn’t display on my Kindle, so I forked the GitHub, knit it to ePub, and converted it to mobi with calibre. I just got to a section that ran getwd() and was totally freaked out for a second thinking that @hadleywickham has the exact same directory structure I do.","2021-08-03 21:45 +0000","5872.0","142.0","0.024182561307901907","0.0","0.0","31.0","37.0","0.0","0.0","74.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422528708850638850","https://twitter.com/LisaDeBruine/status/1422528708850638850","@ravenscimaven Do what you can and submit is really all anyone is expecting. Your committee is unlikely to go through it again with a fine-toothed comb. If you write a cover letter flagging any major changes, they’ll probably just skim that and approve it.","2021-08-03 12:04 +0000","117.0","1.0","0.008547008547008548","0.0","0.0","1.0","0.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422526415594004529","https://twitter.com/LisaDeBruine/status/1422526415594004529","@camjpatrick @dsquintana Oh no. Now it’s in my head again. https://t.co/UmtfkS6r2W","2021-08-03 11:55 +0000","89.0","5.0","0.056179775280898875","0.0","0.0","3.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","26","1","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422511525840003073","https://twitter.com/LisaDeBruine/status/1422511525840003073","@dsquintana Now I’m curious if the no votes are too young, too old, or not online enough.","2021-08-03 10:55 +0000","1386.0","27.0","0.01948051948051948","0.0","4.0","6.0","1.0","0.0","0.0","16.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422189176175403022","https://twitter.com/LisaDeBruine/status/1422189176175403022","@WoTiiToW @GhawinRiver @hoovlet @HarvardHEB Of course, fertility in sterile members of a cooperatively breeding group can be unsuppressed in certain circumstances. And I agree that judging human behaviour based on what other organisms do is pretty ridiculous… glad you finally see it too.","2021-08-02 13:34 +0000","51.0","2.0","0.0392156862745098","0.0","1.0","0.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
"1422180093351407616","https://twitter.com/LisaDeBruine/status/1422180093351407616","@WoTiiToW @GhawinRiver @hoovlet @HarvardHEB Hold on, so worker bees are “abnormal” according to you? Probably you need to update your biological education before you continue.","2021-08-02 12:58 +0000","44.0","2.0","0.045454545454545456","0.0","1.0","0.0","1.0","0.0","0.0","0.0","0.0","0","0","0","0","0","0","0","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-","-"
